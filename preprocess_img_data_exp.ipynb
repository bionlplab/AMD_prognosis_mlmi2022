{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only using CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(output_path, mydata):\n",
    "    with open(output_path, 'wb') as f:\n",
    "        \n",
    "        pickle.dump(mydata, f)\n",
    "        \n",
    "def load_data(data_path):\n",
    "    data = pickle.load(open(data_path, 'rb'))\n",
    "\n",
    "    return data\n",
    "\n",
    "def convert_binary_score(severe_score):\n",
    "    \n",
    "    if severe_score >= 10:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def lower_filepath(filepath):\n",
    "    \n",
    "    filename, fileformat = filepath.split(\".\")\n",
    "    fileformat = fileformat.lower()\n",
    "    \n",
    "    converted_filepath = filename + \".\" + fileformat\n",
    "    \n",
    "    return converted_filepath\n",
    "    \n",
    "def shuffle_data(mydata):\n",
    "    mydata = np.array(mydata)\n",
    "    idx = np.arange(len(mydata))\n",
    "    random.shuffle(idx)\n",
    "    \n",
    "    return mydata[idx]\n",
    "    \n",
    "def filter_available_visits(amd_record_visit):\n",
    "    \n",
    "    filtered_visits = []\n",
    "    \n",
    "    for visit in amd_record_visit:\n",
    "        re_label_test = False\n",
    "        re_data_test = False\n",
    "        le_label_test = False\n",
    "        le_data_test = False\n",
    "        \n",
    "        try:\n",
    "            this_re_severe_score = visit[\"AMDSEVRE\"]\n",
    "            if np.isnan(this_re_severe_score):\n",
    "                re_label_test = False\n",
    "            else:\n",
    "                re_label_test = True\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            this_le_severe_score = visit[\"AMDSEVLE\"]\n",
    "            if np.isnan(this_le_severe_score):\n",
    "                le_label_test = False\n",
    "            else:\n",
    "                le_label_test = True\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            this_re_data = visit[\"RE_IMG\"]\n",
    "            if len(this_re_data) > 0:\n",
    "                re_data_test = True\n",
    "            else:\n",
    "                re_data_test = False\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            this_le_data = visit[\"LE_IMG\"]\n",
    "            if len(this_le_data) > 0:\n",
    "                le_data_test = True\n",
    "            else:\n",
    "                le_data_test = False\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        test_result = re_label_test * le_label_test * re_data_test * le_data_test\n",
    "        \n",
    "        if test_result == 1:\n",
    "            filtered_visits.append(visit)\n",
    "            \n",
    "    return filtered_visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_patient_dict(data_dir, remove_recurrent=True):\n",
    "    \n",
    "    json_data = open(data_dir)\n",
    "    amd_data = json.load(json_data)\n",
    "    patient_dict = dict()\n",
    "    \n",
    "    count_idx = 0\n",
    "    count_patient = 0\n",
    "    count_removed_patient = 0\n",
    "    for idx, record in enumerate(amd_data):\n",
    "        count_patient += 1\n",
    "        if idx % 100 == 0:\n",
    "            print(\"{} patients processed...\".format(count_idx*100))\n",
    "            count_idx += 1\n",
    "        \n",
    "        this_id = record[\"ID2\"]\n",
    "        this_visits = record[\"VISITS\"]\n",
    "        filtered_visits = filter_available_visits(this_visits)\n",
    "        this_re = dict()\n",
    "        this_le = dict()\n",
    "        \n",
    "        re_year = []\n",
    "        le_year = []\n",
    "        re_img = []\n",
    "        le_img = []\n",
    "        re_severe_score = []\n",
    "        le_severe_score = []\n",
    "        re_late_amd = []\n",
    "        le_late_amd = []\n",
    "        \n",
    "        if len(filtered_visits) <= 1:\n",
    "            count_removed_patient += 1\n",
    "            continue\n",
    "            \n",
    "        for i, visit in enumerate(filtered_visits):\n",
    "            \n",
    "            re_year.append(int(visit[\"VISNO\"])/2)\n",
    "            le_year.append(int(visit[\"VISNO\"])/2)\n",
    "            re_img.append(lower_filepath(visit[\"RE_IMG\"]))\n",
    "            le_img.append(lower_filepath(visit[\"LE_IMG\"]))\n",
    "            re_severe_score.append(visit[\"AMDSEVRE\"])\n",
    "            le_severe_score.append(visit[\"AMDSEVLE\"])\n",
    "            re_late_amd.append(convert_binary_score(visit[\"AMDSEVRE\"]))\n",
    "            le_late_amd.append(convert_binary_score(visit[\"AMDSEVLE\"]))\n",
    "            \n",
    "        re_late_amd = np.array(re_late_amd)\n",
    "        le_late_amd = np.array(le_late_amd)\n",
    "        \n",
    "        this_re[\"re_year\"] = re_year\n",
    "        this_re[\"re_img\"] = re_img\n",
    "        this_re[\"re_severe_score\"] = re_severe_score\n",
    "        this_re[\"re_late_amd\"] = re_late_amd\n",
    "        \n",
    "        this_le[\"le_year\"] = le_year\n",
    "        this_le[\"le_img\"] = le_img\n",
    "        this_le[\"le_severe_score\"] = le_severe_score\n",
    "        this_le[\"le_late_amd\"] = le_late_amd\n",
    "        \n",
    "        patient_late_amd_check = np.sum(re_late_amd) + np.sum(le_late_amd)\n",
    "        \n",
    "        if patient_late_amd_check > 0:\n",
    "            patient_late_amd_label = 1\n",
    "        else:\n",
    "            patient_late_amd_label = 0\n",
    "        \n",
    "        patient_dict[this_id] = {\"re\" : this_re, \"le\" : this_le, \"late_amd_label\" : patient_late_amd_label}\n",
    "    \n",
    "    print(\"the number of patients: {}\".format(count_patient))\n",
    "    print(\"{} patients that have no valid visits were excluded\".format(count_removed_patient))\n",
    "    \n",
    "    return patient_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 patients processed...\n",
      "100 patients processed...\n",
      "200 patients processed...\n",
      "300 patients processed...\n",
      "400 patients processed...\n",
      "500 patients processed...\n",
      "600 patients processed...\n",
      "700 patients processed...\n",
      "800 patients processed...\n",
      "900 patients processed...\n",
      "1000 patients processed...\n",
      "1100 patients processed...\n",
      "1200 patients processed...\n",
      "1300 patients processed...\n",
      "1400 patients processed...\n",
      "1500 patients processed...\n",
      "1600 patients processed...\n",
      "1700 patients processed...\n",
      "1800 patients processed...\n",
      "1900 patients processed...\n",
      "2000 patients processed...\n",
      "2100 patients processed...\n",
      "2200 patients processed...\n",
      "2300 patients processed...\n",
      "2400 patients processed...\n",
      "2500 patients processed...\n",
      "2600 patients processed...\n",
      "2700 patients processed...\n",
      "2800 patients processed...\n",
      "2900 patients processed...\n",
      "3000 patients processed...\n",
      "3100 patients processed...\n",
      "3200 patients processed...\n",
      "3300 patients processed...\n",
      "3400 patients processed...\n",
      "3500 patients processed...\n",
      "3600 patients processed...\n",
      "3700 patients processed...\n",
      "3800 patients processed...\n",
      "3900 patients processed...\n",
      "4000 patients processed...\n",
      "4100 patients processed...\n",
      "4200 patients processed...\n",
      "4300 patients processed...\n",
      "4400 patients processed...\n",
      "4500 patients processed...\n",
      "4600 patients processed...\n",
      "4700 patients processed...\n",
      "the number of patients: 4757\n",
      "442 patients that have no valid visits were excluded\n"
     ]
    }
   ],
   "source": [
    "patient_dict = build_patient_dict(\"/home/jl5307/current_research/AMD_prediction/data/AREDS_participants_amd3.json\", remove_recurrent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'re': {'re_year': [0.0, 2.0, 3.0, 4.0, 5.0, 7.0, 8.0, 10.0],\n",
       "  're_img': ['51685 QUA F2 RE LS.jpg',\n",
       "   '51685 04 F2 RE LS.jpg',\n",
       "   '51685 06 F2 RE LS.jpg',\n",
       "   '51685 08 F2 RE LS.jpg',\n",
       "   '51685 10 F2 RE LS.jpg',\n",
       "   '51685 14 F2 RE LS.jpg',\n",
       "   '51685 16 F2 RE LS.jpg',\n",
       "   '51685 20 F2 RE LS.jpg'],\n",
       "  're_severe_score': [4.0, 3.0, 4.0, 4.0, 5.0, 6.0, 8.0, 7.0],\n",
       "  're_late_amd': array([0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " 'le': {'le_year': [0.0, 2.0, 3.0, 4.0, 5.0, 7.0, 8.0, 10.0],\n",
       "  'le_img': ['51685 QUA F2 LE LS.jpg',\n",
       "   '51685 04 F2 LE LS.jpg',\n",
       "   '51685 06 F2 LE LS.jpg',\n",
       "   '51685 08 F2 LE LS.jpg',\n",
       "   '51685 10 F2 LE LS.jpg',\n",
       "   '51685 14 F2 LE LS.jpg',\n",
       "   '51685 16 F2 LE LS.jpg',\n",
       "   '51685 20 F2 LE LS.jpg'],\n",
       "  'le_severe_score': [1.0, 3.0, 3.0, 2.0, 4.0, 6.0, 8.0, 6.0],\n",
       "  'le_late_amd': array([0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " 'late_amd_label': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_dict[\"1001\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split patient dict into train-validation-test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_patient_dict(patient_dict, train_size, validation_size, test_size):\n",
    "    \n",
    "    late_amd_patient_list = []\n",
    "    non_late_amd_patient_list = []\n",
    "    \n",
    "    for pid, value in patient_dict.items():\n",
    "        \n",
    "        if value[\"late_amd_label\"] == 1:\n",
    "            late_amd_patient_list.append(pid)\n",
    "        else:\n",
    "            non_late_amd_patient_list.append(pid)\n",
    "            \n",
    "    late_amd_patient_list = shuffle_data(late_amd_patient_list)\n",
    "    non_late_amd_patient_list = shuffle_data(non_late_amd_patient_list)\n",
    "    \n",
    "    print(\"the number of patients have late-AMD at least one eye: {}\".format(len(late_amd_patient_list)))\n",
    "    print(\"the number of patients do have late-AMD at least one eye: {}\".format(len(non_late_amd_patient_list)))\n",
    "    \n",
    "    ## train set\n",
    "    train_late_amd_patient_list = late_amd_patient_list[:int(np.floor(len(late_amd_patient_list)*train_size))]\n",
    "    train_non_late_amd_patient_list = non_late_amd_patient_list[:int(np.floor(len(non_late_amd_patient_list)*train_size))]  \n",
    "    \n",
    "    ## test and validation set\n",
    "    test_validation_late_amd_patient_list = late_amd_patient_list[int(np.floor(len(late_amd_patient_list)*train_size)):]\n",
    "    test_validation_non_late_amd_patient_list = non_late_amd_patient_list[int(np.floor(len(non_late_amd_patient_list)*train_size)):]\n",
    "    \n",
    "    late_amd_test_validation_slicing_ind = int(np.floor(len(test_validation_late_amd_patient_list) * (test_size / (validation_size+test_size))))\n",
    "    non_late_amd_test_validation_slicing_ind = int(np.floor(len(test_validation_non_late_amd_patient_list) * (test_size / (validation_size+test_size))))\n",
    "    \n",
    "    test_late_amd_patient_list = test_validation_late_amd_patient_list[:late_amd_test_validation_slicing_ind]\n",
    "    test_non_late_amd_patient_list = test_validation_non_late_amd_patient_list[:non_late_amd_test_validation_slicing_ind]\n",
    "    \n",
    "    validation_late_amd_patient_list = test_validation_late_amd_patient_list[late_amd_test_validation_slicing_ind:]\n",
    "    validation_non_late_amd_patient_list = test_validation_non_late_amd_patient_list[non_late_amd_test_validation_slicing_ind:]\n",
    "\n",
    "    train_pid_list = []\n",
    "    validation_pid_list = []\n",
    "    test_pid_list = []\n",
    "\n",
    "    train_pid_list.extend(train_late_amd_patient_list)\n",
    "    train_pid_list.extend(train_non_late_amd_patient_list)\n",
    "    validation_pid_list.extend(validation_late_amd_patient_list)\n",
    "    validation_pid_list.extend(validation_non_late_amd_patient_list)\n",
    "    test_pid_list.extend(test_late_amd_patient_list)\n",
    "    test_pid_list.extend(test_non_late_amd_patient_list)\n",
    "    \n",
    "    train_pid_list = shuffle_data(train_pid_list)\n",
    "    validation_pid_list = shuffle_data(validation_pid_list)\n",
    "    test_pid_list = shuffle_data(test_pid_list)\n",
    "\n",
    "    train_set = dict()\n",
    "    validation_set = dict()\n",
    "    test_set = dict()\n",
    "    \n",
    "    for pid in train_pid_list:\n",
    "        train_set[pid] = patient_dict[pid]\n",
    "    \n",
    "    for pid in validation_pid_list:\n",
    "        validation_set[pid] = patient_dict[pid]\n",
    "        \n",
    "    for pid in test_pid_list:\n",
    "        test_set[pid] = patient_dict[pid]\n",
    "        \n",
    "    print(len(train_set))\n",
    "    print(len(validation_set))\n",
    "    print(len(test_set))\n",
    "    \n",
    "    return {\"train_set\" : train_set, \"validation_set\" : validation_set, \"test_set\" : test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of patients have late-AMD at least one eye: 1223\n",
      "the number of patients do have late-AMD at least one eye: 3092\n",
      "3020\n",
      "648\n",
      "647\n"
     ]
    }
   ],
   "source": [
    "splitted_patient_dict = split_patient_dict(patient_dict, 0.7, 0.15, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/splitted_patient_dict.pkl\", splitted_patient_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_patient_dict = load_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/splitted_patient_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'re': {'re_year': [0.0,\n",
       "   2.0,\n",
       "   3.0,\n",
       "   4.0,\n",
       "   5.0,\n",
       "   6.0,\n",
       "   7.0,\n",
       "   8.5,\n",
       "   9.0,\n",
       "   10.0,\n",
       "   11.0,\n",
       "   12.0],\n",
       "  're_img': ['56516 QUA F2 RE LS.jpg',\n",
       "   '56516 04 F2 RE LS.jpg',\n",
       "   '56516 06 F2 RE LS.jpg',\n",
       "   '56516 08 F2 RE LS.jpg',\n",
       "   '56516 10 F2 RE LS.jpg',\n",
       "   '56516 12 F2 RE LS.jpg',\n",
       "   '56516 14 F2 RE LS.jpg',\n",
       "   '56516 17 F2 RE LS.jpg',\n",
       "   '56516 18 F2 RE LS.jpg',\n",
       "   '56516 20 F2 RE LS.jpg',\n",
       "   '56516 22 F2 RE LS.jpg',\n",
       "   '56516 24 F2 RE LS.jpg'],\n",
       "  're_severe_score': [4.0,\n",
       "   5.0,\n",
       "   5.0,\n",
       "   6.0,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   4.0,\n",
       "   8.0,\n",
       "   5.0,\n",
       "   8.0,\n",
       "   8.0,\n",
       "   8.0],\n",
       "  're_late_amd': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " 'le': {'le_year': [0.0,\n",
       "   2.0,\n",
       "   3.0,\n",
       "   4.0,\n",
       "   5.0,\n",
       "   6.0,\n",
       "   7.0,\n",
       "   8.5,\n",
       "   9.0,\n",
       "   10.0,\n",
       "   11.0,\n",
       "   12.0],\n",
       "  'le_img': ['56516 QUA F2 LE LS.jpg',\n",
       "   '56516 04 F2 LE LS.jpg',\n",
       "   '56516 06 F2 LE LS.jpg',\n",
       "   '56516 08 F2 LE LS.jpg',\n",
       "   '56516 10 F2 LE LS.jpg',\n",
       "   '56516 12 F2 LE LS.jpg',\n",
       "   '56516 14 F2 LE LS.jpg',\n",
       "   '56516 17 F2 LE LS.jpg',\n",
       "   '56516 18 F2 LE LS.jpg',\n",
       "   '56516 20 F2 LE LS.jpg',\n",
       "   '56516 22 F2 LE LS.jpg',\n",
       "   '56516 24 F2 LE LS.jpg'],\n",
       "  'le_severe_score': [4.0,\n",
       "   5.0,\n",
       "   4.0,\n",
       "   5.0,\n",
       "   2.0,\n",
       "   8.0,\n",
       "   4.0,\n",
       "   8.0,\n",
       "   8.0,\n",
       "   8.0,\n",
       "   4.0,\n",
       "   8.0],\n",
       "  'le_late_amd': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " 'late_amd_label': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_patient_dict[\"train_set\"][\"G273\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_img_year(patient_set_dict):\n",
    "    \n",
    "    unique_year = set()\n",
    "    img_year_list = []\n",
    "    img_year_dict = dict()\n",
    "    \n",
    "    for pid, value in patient_set_dict.items():\n",
    "        \n",
    "        re_year = value[\"re\"][\"re_year\"]\n",
    "        le_year = value[\"le\"][\"le_year\"]\n",
    "        re_late_amd_list = value[\"re\"][\"re_late_amd\"]\n",
    "        le_late_amd_list = value[\"le\"][\"le_late_amd\"]\n",
    "        \n",
    "        unique_year.update(set(np.unique(re_year)))\n",
    "        unique_year.update(set(np.unique(le_year)))\n",
    "        \n",
    "        for year, re_label in zip(re_year, re_late_amd_list):\n",
    "            img_year_list.append((year, re_label))\n",
    "            \n",
    "        for year, le_label in zip(re_year, le_late_amd_list):\n",
    "            img_year_list.append((year, le_label))\n",
    "            \n",
    "    for year in unique_year:\n",
    "        img_year_dict[year] = dict()\n",
    "        img_year_dict[year][0] = 0\n",
    "        img_year_dict[year][1] = 0\n",
    "        \n",
    "    for (year, label) in img_year_list:\n",
    "        img_year_dict[year][label] += 1\n",
    "            \n",
    "    return img_year_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: {0: 1030, 1: 84},\n",
       " 0.5: {0: 26, 1: 24},\n",
       " 2.0: {0: 953, 1: 119},\n",
       " 3.0: {0: 1047, 1: 141},\n",
       " 4.0: {0: 887, 1: 113},\n",
       " 5.0: {0: 940, 1: 154},\n",
       " 6.0: {0: 706, 1: 88},\n",
       " 1.0: {0: 40, 1: 22},\n",
       " 7.0: {0: 698, 1: 126},\n",
       " 8.0: {0: 481, 1: 67},\n",
       " 9.0: {0: 565, 1: 123},\n",
       " 1.5: {0: 28, 1: 12},\n",
       " 2.5: {0: 27, 1: 7},\n",
       " 5.5: {0: 54, 1: 4},\n",
       " 10.0: {0: 482, 1: 66},\n",
       " 12.0: {0: 115, 1: 23},\n",
       " 7.5: {0: 53, 1: 7},\n",
       " 11.0: {0: 330, 1: 76},\n",
       " 9.5: {0: 4, 1: 0},\n",
       " 3.5: {0: 21, 1: 3},\n",
       " 13.0: {0: 1, 1: 1},\n",
       " 4.5: {0: 23, 1: 11},\n",
       " 6.5: {0: 67, 1: 7},\n",
       " 8.5: {0: 5, 1: 3}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_img_year(splitted_patient_dict[\"test_set\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data dictionary for amd detection task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_binary_detection_data_dict(splitted_patient_dict):\n",
    "    \n",
    "    train_set = splitted_patient_dict[\"train_set\"]\n",
    "    validation_set = splitted_patient_dict[\"validation_set\"]\n",
    "    test_set = splitted_patient_dict[\"test_set\"]\n",
    "    \n",
    "    train_eye_dict = dict()\n",
    "    validation_eye_dict = dict()\n",
    "    test_eye_dict = dict()\n",
    "    \n",
    "    late_amd_img_count = 0\n",
    "    non_late_amd_img_count = 0\n",
    "    \n",
    "    # train set\n",
    "    for pid, value in train_set.items():\n",
    "        \n",
    "        re_img_list = value[\"re\"][\"re_img\"]\n",
    "        re_label_list = value[\"re\"][\"re_late_amd\"]\n",
    "        \n",
    "        if len(re_img_list) != len(re_label_list):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        le_img_list = value[\"le\"][\"le_img\"]\n",
    "        le_label_list = value[\"le\"][\"le_late_amd\"]\n",
    "        \n",
    "        if len(le_img_list) != len(le_label_list):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        for img, label in zip(re_img_list, re_label_list):\n",
    "            if label == 1:\n",
    "                late_amd_img_count +=1 \n",
    "            else: \n",
    "                non_late_amd_img_count += 1\n",
    "                \n",
    "            train_eye_dict[img] = label\n",
    "        \n",
    "        for img, label in zip(le_img_list, le_label_list):\n",
    "            if label == 1:\n",
    "                late_amd_img_count +=1 \n",
    "            else: \n",
    "                non_late_amd_img_count += 1\n",
    "            train_eye_dict[img] = label\n",
    "            \n",
    "    # validation set\n",
    "    for pid, value in validation_set.items():\n",
    "        \n",
    "        re_img_list = value[\"re\"][\"re_img\"]\n",
    "        re_label_list = value[\"re\"][\"re_late_amd\"]\n",
    "        \n",
    "        if len(re_img_list) != len(re_label_list):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        le_img_list = value[\"le\"][\"le_img\"]\n",
    "        le_label_list = value[\"le\"][\"le_late_amd\"]\n",
    "        \n",
    "        if len(le_img_list) != len(le_label_list):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        for img, label in zip(re_img_list, re_label_list):\n",
    "            validation_eye_dict[img] = label\n",
    "        \n",
    "        for img, label in zip(le_img_list, le_label_list):\n",
    "            validation_eye_dict[img] = label\n",
    "            \n",
    "    # test set\n",
    "    for pid, value in test_set.items():\n",
    "        \n",
    "        re_img_list = value[\"re\"][\"re_img\"]\n",
    "        re_label_list = value[\"re\"][\"re_late_amd\"]\n",
    "        \n",
    "        if len(re_img_list) != len(re_label_list):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        le_img_list = value[\"le\"][\"le_img\"]\n",
    "        le_label_list = value[\"le\"][\"le_late_amd\"]\n",
    "        \n",
    "        if len(le_img_list) != len(le_label_list):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        for img, label in zip(re_img_list, re_label_list):\n",
    "            test_eye_dict[img] = label\n",
    "        \n",
    "        for img, label in zip(le_img_list, le_label_list):\n",
    "            test_eye_dict[img] = label\n",
    "    \n",
    "    print(\"late-AMD class ratio: {}\".format(late_amd_img_count / (late_amd_img_count+non_late_amd_img_count)))\n",
    "    print(\"non late-AMD class ratio: {}\".format(non_late_amd_img_count / (late_amd_img_count+non_late_amd_img_count)))\n",
    "    \n",
    "    return {\"train_set\" : train_eye_dict, \"validation_set\" : validation_eye_dict, \"test_set\" : test_eye_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "late-AMD class ratio: 0.12817325307511124\n",
      "non late-AMD class ratio: 0.8718267469248888\n"
     ]
    }
   ],
   "source": [
    "binary_detection_data_dict = build_binary_detection_data_dict(splitted_patient_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/binary_detection_data_dict.pkl\", binary_detection_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_detection_data_dict = load_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/binary_detection_data_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data dictionary for within 2-year binary prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_patient_dict = load_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/splitted_patient_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_year(data_set_dict):\n",
    "    \n",
    "    unique_year_set = set()\n",
    "    \n",
    "    for pid, value in data_set_dict.items():\n",
    "        re_unique_year = set(np.unique(value[\"re\"][\"re_year\"]))\n",
    "        le_unique_year = set(np.unique(value[\"le\"][\"le_year\"]))\n",
    "        unique_year_set.update(re_unique_year)\n",
    "        unique_year_set.update(le_unique_year)\n",
    "        \n",
    "    return list(unique_year_set)\n",
    "\n",
    "def build_binary_prediction_data_dict(splitted_patient_dict, timestamp_delta, remove_recurrent=True):\n",
    "    \n",
    "    train_set = splitted_patient_dict[\"train_set\"]\n",
    "    validation_set = splitted_patient_dict[\"validation_set\"]\n",
    "    test_set = splitted_patient_dict[\"test_set\"]\n",
    "    \n",
    "    train_eye_dict = dict()\n",
    "    validation_eye_dict = dict()\n",
    "    test_eye_dict = dict()\n",
    "    \n",
    "    # train set\n",
    "    for pid, value in train_set.items():\n",
    "        \n",
    "        re_year = np.array(value[\"re\"][\"re_year\"])\n",
    "        re_img_list = value[\"re\"][\"re_img\"]\n",
    "        re_severe_score = value[\"re\"][\"re_severe_score\"]\n",
    "        re_late_amd = np.array(value[\"re\"][\"re_late_amd\"])\n",
    "        \n",
    "        if len(re_img_list) != len(re_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        le_year = np.array(value[\"le\"][\"le_year\"])\n",
    "        le_img_list = value[\"le\"][\"le_img\"]\n",
    "        le_severe_score = value[\"le\"][\"le_severe_score\"]\n",
    "        le_late_amd = np.array(value[\"le\"][\"le_late_amd\"])\n",
    "        \n",
    "        if len(le_img_list) != len(le_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        if remove_recurrent: # remove recurrent late-AMD labels\n",
    "            \n",
    "            if np.sum(re_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                re_first_late_amd_idx = np.where(re_late_amd == 1)[0][0]\n",
    "                re_year = re_year[:re_first_late_amd_idx+1]\n",
    "                re_img_list = re_img_list[:re_first_late_amd_idx+1]\n",
    "                re_severe_score = re_severe_score[:re_first_late_amd_idx+1]\n",
    "                re_late_amd = re_late_amd[:re_first_late_amd_idx+1]\n",
    "                \n",
    "            if np.sum(le_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                le_first_late_amd_idx = np.where(le_late_amd == 1)[0][0]\n",
    "                le_year = le_year[:le_first_late_amd_idx+1]\n",
    "                le_img_list = le_img_list[:le_first_late_amd_idx+1]\n",
    "                le_severe_score = le_severe_score[:le_first_late_amd_idx+1]\n",
    "                le_late_amd = le_late_amd[:le_first_late_amd_idx+1]\n",
    "\n",
    "        if len(re_year) >= 2: \n",
    "            # check there are at least more than one \n",
    "        \n",
    "            for idx, year in enumerate(re_year):\n",
    "                eye_img = re_img_list[idx]\n",
    "                label_year = year + timestamp_delta\n",
    "                label_ind1 = np.where(re_year > year, True, False)\n",
    "                label_ind2 = np.where(re_year <= label_year, True, False)\n",
    "                label_ind = label_ind1 * label_ind2\n",
    "            \n",
    "                if np.sum(label_ind) > 0:\n",
    "                    train_eye_dict[eye_img] = int(np.max(re_late_amd[label_ind]))\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "        if len(le_year) >= 2: \n",
    "            # check there are at least more than one \n",
    "            \n",
    "            for idx, year in enumerate(le_year):\n",
    "                eye_img = le_img_list[idx]\n",
    "                label_year = year + timestamp_delta\n",
    "                label_ind1 = np.where(le_year > year, True, False)\n",
    "                label_ind2 = np.where(le_year <= label_year, True, False)\n",
    "                label_ind = label_ind1 * label_ind2\n",
    "            \n",
    "                if np.sum(label_ind) > 0:\n",
    "                    train_eye_dict[eye_img] = int(np.max(le_late_amd[label_ind]))\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "    # validation set\n",
    "    for pid, value in validation_set.items():\n",
    "        \n",
    "        re_year = np.array(value[\"re\"][\"re_year\"])\n",
    "        re_img_list = value[\"re\"][\"re_img\"]\n",
    "        re_severe_score = value[\"re\"][\"re_severe_score\"]\n",
    "        re_late_amd = np.array(value[\"re\"][\"re_late_amd\"])\n",
    "        \n",
    "        if len(re_img_list) != len(re_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        le_year = np.array(value[\"le\"][\"le_year\"])\n",
    "        le_img_list = value[\"le\"][\"le_img\"]\n",
    "        le_severe_score = value[\"le\"][\"le_severe_score\"]\n",
    "        le_late_amd = np.array(value[\"le\"][\"le_late_amd\"])\n",
    "        \n",
    "        if len(le_img_list) != len(le_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        if remove_recurrent: # remove recurrent late-AMD labels\n",
    "            \n",
    "            if np.sum(re_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                re_first_late_amd_idx = np.where(re_late_amd == 1)[0][0]\n",
    "                re_year = re_year[:re_first_late_amd_idx+1]\n",
    "                re_img_list = re_img_list[:re_first_late_amd_idx+1]\n",
    "                re_severe_score = re_severe_score[:re_first_late_amd_idx+1]\n",
    "                re_late_amd = re_late_amd[:re_first_late_amd_idx+1]\n",
    "                \n",
    "            if np.sum(le_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                le_first_late_amd_idx = np.where(le_late_amd == 1)[0][0]\n",
    "                le_year = le_year[:le_first_late_amd_idx+1]\n",
    "                le_img_list = le_img_list[:le_first_late_amd_idx+1]\n",
    "                le_severe_score = le_severe_score[:le_first_late_amd_idx+1]\n",
    "                le_late_amd = le_late_amd[:le_first_late_amd_idx+1]\n",
    "\n",
    "        if len(re_year) >= 2: \n",
    "        \n",
    "            for idx, year in enumerate(re_year):\n",
    "                eye_img = re_img_list[idx]\n",
    "                label_year = year + timestamp_delta\n",
    "                label_ind1 = np.where(re_year > year, True, False)\n",
    "                label_ind2 = np.where(re_year <= label_year, True, False)\n",
    "                label_ind = label_ind1 * label_ind2\n",
    "            \n",
    "                if np.sum(label_ind) > 0:\n",
    "                    validation_eye_dict[eye_img] = int(np.max(re_late_amd[label_ind]))\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "        if len(le_year) >= 2: \n",
    "                \n",
    "            for idx, year in enumerate(le_year):\n",
    "                eye_img = le_img_list[idx]\n",
    "                label_year = year + timestamp_delta\n",
    "                label_ind1 = np.where(le_year > year, True, False)\n",
    "                label_ind2 = np.where(le_year <= label_year, True, False)\n",
    "                label_ind = label_ind1 * label_ind2\n",
    "            \n",
    "                if np.sum(label_ind) > 0:\n",
    "                    validation_eye_dict[eye_img] = int(np.max(le_late_amd[label_ind]))\n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "    # test set\n",
    "    unique_year = count_unique_year(test_set)\n",
    "    \n",
    "    for y in unique_year:\n",
    "        test_eye_dict[y] = dict()\n",
    "\n",
    "    for pid, value in test_set.items():\n",
    "        \n",
    "        re_year = np.array(value[\"re\"][\"re_year\"])\n",
    "        re_img_list = value[\"re\"][\"re_img\"]\n",
    "        re_severe_score = value[\"re\"][\"re_severe_score\"]\n",
    "        re_late_amd = np.array(value[\"re\"][\"re_late_amd\"])\n",
    "        \n",
    "        if len(re_img_list) != len(re_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        le_year = np.array(value[\"le\"][\"le_year\"])\n",
    "        le_img_list = value[\"le\"][\"le_img\"]\n",
    "        le_severe_score = value[\"le\"][\"le_severe_score\"]\n",
    "        le_late_amd = np.array(value[\"le\"][\"le_late_amd\"])\n",
    "        \n",
    "        if len(le_img_list) != len(le_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        if remove_recurrent: # remove recurrent late-AMD labels\n",
    "            \n",
    "            if np.sum(re_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                re_first_late_amd_idx = np.where(re_late_amd == 1)[0][0]\n",
    "                re_year = re_year[:re_first_late_amd_idx+1]\n",
    "                re_img_list = re_img_list[:re_first_late_amd_idx+1]\n",
    "                re_severe_score = re_severe_score[:re_first_late_amd_idx+1]\n",
    "                re_late_amd = re_late_amd[:re_first_late_amd_idx+1]\n",
    "                \n",
    "            if np.sum(le_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                le_first_late_amd_idx = np.where(le_late_amd == 1)[0][0]\n",
    "                le_year = le_year[:le_first_late_amd_idx+1]\n",
    "                le_img_list = le_img_list[:le_first_late_amd_idx+1]\n",
    "                le_severe_score = le_severe_score[:le_first_late_amd_idx+1]\n",
    "                le_late_amd = le_late_amd[:le_first_late_amd_idx+1]\n",
    "                \n",
    "        if len(re_year) >= 2: \n",
    "        \n",
    "            for idx, year in enumerate(re_year):\n",
    "                eye_img = re_img_list[idx]\n",
    "                label_year = year + timestamp_delta\n",
    "                label_ind1 = np.where(re_year > year, True, False)\n",
    "                label_ind2 = np.where(re_year <= label_year, True, False)\n",
    "                label_ind = label_ind1 * label_ind2\n",
    "            \n",
    "                if np.sum(label_ind) > 0:\n",
    "                    test_eye_dict[year][eye_img] = int(np.max(re_late_amd[label_ind]))\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        if len(le_year) >= 2: \n",
    "                \n",
    "            for idx, year in enumerate(le_year):\n",
    "                eye_img = le_img_list[idx]\n",
    "                label_year = year + timestamp_delta\n",
    "                label_ind1 = np.where(le_year > year, True, False)\n",
    "                label_ind2 = np.where(le_year <= label_year, True, False)\n",
    "                label_ind = label_ind1 * label_ind2\n",
    "            \n",
    "                if np.sum(label_ind) > 0:\n",
    "                    test_eye_dict[year][eye_img] = int(np.max(le_late_amd[label_ind]))\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "    # count the number of data\n",
    "    print(\"the number of data in train set: {}\".format(len(train_eye_dict)))\n",
    "    print(\"the number of data in validation set: {}\".format(len(validation_eye_dict)))\n",
    "    \n",
    "    test_data_count = 0\n",
    "    for year, value in test_eye_dict.items():\n",
    "        test_data_count += len(value)\n",
    "        \n",
    "    print(\"the number of data in test set: {}\".format(test_data_count))\n",
    "          \n",
    "    return {\"train_set\" : train_eye_dict, \"validation_set\" : validation_eye_dict, \"test_set\" : test_eye_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of data in train set: 33870\n",
      "the number of data in validation set: 7213\n",
      "the number of data in test set: 7264\n"
     ]
    }
   ],
   "source": [
    "binary_prediction_data_dict = build_binary_prediction_data_dict(splitted_patient_dict, timestamp_delta=2, remove_recurrent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/binary_prediction_data_dict.pkl\", binary_prediction_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weight(data_dict):\n",
    "    \n",
    "    \n",
    "    training_set_dict = data_dict[\"train_set\"]\n",
    "    \n",
    "    late_amd_count = 0\n",
    "    non_late_amd_count = 0\n",
    "    \n",
    "    for img, label in training_set_dict.items():\n",
    "        if label == 1:\n",
    "            late_amd_count += 1\n",
    "        else:\n",
    "            non_late_amd_count += 1\n",
    "            \n",
    "    total_count = late_amd_count + non_late_amd_count\n",
    "    \n",
    "    assert total_count == len(training_set_dict), \"data length does not match\"\n",
    "    \n",
    "    print(\"class weight for label [0, 1]: [{zero}, {one}]\".format(zero=late_amd_count/total_count, one=non_late_amd_count/total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weight for label [0, 1]: [0.025214053734868614, 0.9747859462651314]\n"
     ]
    }
   ],
   "source": [
    "calculate_class_weight(binary_prediction_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data dictionary for within 2-year longitudinal prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'re_year': [0.0, 0.5, 2.0, 3.0, 3.5, 4.0, 5.0, 6.0, 7.0, 9.0, 10.0],\n",
       " 're_img': ['58682 QUA F2 RE LS.jpg',\n",
       "  '58682 01 F2 RE LS.jpg',\n",
       "  '58682 04 F2 RE LS.jpg',\n",
       "  '58682 06 F2 RE LS.jpg',\n",
       "  '58682 07 F2 RE LS.jpg',\n",
       "  '58682 08 F2 RE LS.jpg',\n",
       "  '58682 10 F2 RE LS.jpg',\n",
       "  '58682 12 F2 RE LS.jpg',\n",
       "  '58682 14 F2 RE LS.jpg',\n",
       "  '58682 18 F2 RE LS.jpg',\n",
       "  '58682 20 F2 RE LS.jpg'],\n",
       " 're_severe_score': [7.0,\n",
       "  6.0,\n",
       "  6.0,\n",
       "  11.0,\n",
       "  11.0,\n",
       "  11.0,\n",
       "  11.0,\n",
       "  11.0,\n",
       "  11.0,\n",
       "  11.0,\n",
       "  11.0],\n",
       " 're_late_amd': array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_patient_dict[\"train_set\"][\"5535\"][\"re\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_longitudinal_prediction_data_dict(splitted_patient_dict, timestamp_delta, remove_recurrent=True, return_statistics=True):\n",
    "    \n",
    "    train_set = splitted_patient_dict[\"train_set\"]\n",
    "    validation_set = splitted_patient_dict[\"validation_set\"]\n",
    "    test_set = splitted_patient_dict[\"test_set\"]\n",
    "    \n",
    "    train_eye_dict = dict()\n",
    "    validation_eye_dict = dict()\n",
    "    test_eye_dict = dict()\n",
    "    \n",
    "    train_eye_list = []\n",
    "    train_label_list = []\n",
    "    validation_eye_list = []\n",
    "    validation_label_list = []\n",
    "    \n",
    "    train_set_year_distribution = []\n",
    "    validation_set_year_distribution = []\n",
    "    \n",
    "    # train set\n",
    "    for pid, value in train_set.items():\n",
    "        \n",
    "        re_year = np.array(value[\"re\"][\"re_year\"])\n",
    "        re_img_list = value[\"re\"][\"re_img\"]\n",
    "        re_severe_score = value[\"re\"][\"re_severe_score\"]\n",
    "        re_late_amd = np.array(value[\"re\"][\"re_late_amd\"])\n",
    "        \n",
    "        if len(re_img_list) != len(re_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        le_year = np.array(value[\"le\"][\"le_year\"])\n",
    "        le_img_list = value[\"le\"][\"le_img\"]\n",
    "        le_severe_score = value[\"le\"][\"le_severe_score\"]\n",
    "        le_late_amd = np.array(value[\"le\"][\"le_late_amd\"])\n",
    "        \n",
    "        if len(le_img_list) != len(le_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        if remove_recurrent: # remove recurrent late-AMD labels\n",
    "            \n",
    "            if np.sum(re_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                re_first_late_amd_idx = np.where(re_late_amd == 1)[0][0]\n",
    "                re_year = re_year[:re_first_late_amd_idx+1]\n",
    "                re_img_list = re_img_list[:re_first_late_amd_idx+1]\n",
    "                re_severe_score = re_severe_score[:re_first_late_amd_idx+1]\n",
    "                re_late_amd = re_late_amd[:re_first_late_amd_idx+1]\n",
    "                \n",
    "            if np.sum(le_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                le_first_late_amd_idx = np.where(le_late_amd == 1)[0][0]\n",
    "                le_year = le_year[:le_first_late_amd_idx+1]\n",
    "                le_img_list = le_img_list[:le_first_late_amd_idx+1]\n",
    "                le_severe_score = le_severe_score[:le_first_late_amd_idx+1]\n",
    "                le_late_amd = le_late_amd[:le_first_late_amd_idx+1]\n",
    "\n",
    "        if len(re_year) >= 2: \n",
    "        \n",
    "            for idx, year in enumerate(re_year):\n",
    "                longitudinal_eye_img_list = re_img_list[:(idx+1)]\n",
    "                label_year = year + timestamp_delta\n",
    "                label_ind1 = np.where(re_year > year, True, False)\n",
    "                label_ind2 = np.where(re_year <= label_year, True, False)\n",
    "                label_ind = label_ind1 * label_ind2\n",
    "            \n",
    "                if np.sum(label_ind) > 0:\n",
    "                    train_eye_list.append(longitudinal_eye_img_list)\n",
    "                    train_label_list.append(int(np.max(re_late_amd[label_ind])))\n",
    "                    train_set_year_distribution.append(year)\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "        if len(le_year) >= 2: \n",
    "                \n",
    "            for idx, year in enumerate(le_year):\n",
    "                longitudinal_eye_img_list = le_img_list[:(idx+1)]\n",
    "                label_year = year + timestamp_delta\n",
    "                label_ind1 = np.where(le_year > year, True, False)\n",
    "                label_ind2 = np.where(le_year <= label_year, True, False)\n",
    "                label_ind = label_ind1 * label_ind2\n",
    "            \n",
    "                if np.sum(label_ind) > 0:\n",
    "                    train_eye_list.append(longitudinal_eye_img_list)\n",
    "                    train_label_list.append(int(np.max(le_late_amd[label_ind])))\n",
    "                    train_set_year_distribution.append(year)\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "    train_eye_dict[\"eye_list\"] = train_eye_list\n",
    "    train_eye_dict[\"label_list\"] = train_label_list\n",
    "                    \n",
    "    # validation set\n",
    "    for pid, value in validation_set.items():\n",
    "        \n",
    "        re_year = np.array(value[\"re\"][\"re_year\"])\n",
    "        re_img_list = value[\"re\"][\"re_img\"]\n",
    "        re_severe_score = value[\"re\"][\"re_severe_score\"]\n",
    "        re_late_amd = np.array(value[\"re\"][\"re_late_amd\"])\n",
    "        \n",
    "        if len(re_img_list) != len(re_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        le_year = np.array(value[\"le\"][\"le_year\"])\n",
    "        le_img_list = value[\"le\"][\"le_img\"]\n",
    "        le_severe_score = value[\"le\"][\"le_severe_score\"]\n",
    "        le_late_amd = np.array(value[\"le\"][\"le_late_amd\"])\n",
    "        \n",
    "        if len(le_img_list) != len(le_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        if remove_recurrent: # remove recurrent late-AMD labels\n",
    "            \n",
    "            if np.sum(re_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                re_first_late_amd_idx = np.where(re_late_amd == 1)[0][0]\n",
    "                re_year = re_year[:re_first_late_amd_idx+1]\n",
    "                re_img_list = re_img_list[:re_first_late_amd_idx+1]\n",
    "                re_severe_score = re_severe_score[:re_first_late_amd_idx+1]\n",
    "                re_late_amd = re_late_amd[:re_first_late_amd_idx+1]\n",
    "                \n",
    "            if np.sum(le_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                le_first_late_amd_idx = np.where(le_late_amd == 1)[0][0]\n",
    "                le_year = le_year[:le_first_late_amd_idx+1]\n",
    "                le_img_list = le_img_list[:le_first_late_amd_idx+1]\n",
    "                le_severe_score = le_severe_score[:le_first_late_amd_idx+1]\n",
    "                le_late_amd = le_late_amd[:le_first_late_amd_idx+1]\n",
    "\n",
    "        if len(re_year) >= 2: \n",
    "        \n",
    "            for idx, year in enumerate(re_year):\n",
    "                longitudinal_eye_img_list = re_img_list[:(idx+1)]\n",
    "                label_year = year + timestamp_delta\n",
    "                label_ind1 = np.where(re_year > year, True, False)\n",
    "                label_ind2 = np.where(re_year <= label_year, True, False)\n",
    "                label_ind = label_ind1 * label_ind2\n",
    "            \n",
    "                if np.sum(label_ind) > 0:\n",
    "                    validation_eye_list.append(longitudinal_eye_img_list)\n",
    "                    validation_label_list.append(int(np.max(re_late_amd[label_ind])))\n",
    "                    validation_set_year_distribution.append(year)\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "        if len(le_year) >= timestamp_delta: \n",
    "                \n",
    "            for idx, year in enumerate(le_year):\n",
    "                longitudinal_eye_img_list = le_img_list[:(idx+1)]\n",
    "                label_year = year + timestamp_delta\n",
    "                label_ind1 = np.where(le_year > year, True, False)\n",
    "                label_ind2 = np.where(le_year <= label_year, True, False)\n",
    "                label_ind = label_ind1 * label_ind2\n",
    "            \n",
    "                if np.sum(label_ind) > 0:\n",
    "                    validation_eye_list.append(longitudinal_eye_img_list)\n",
    "                    validation_label_list.append(int(np.max(le_late_amd[label_ind])))\n",
    "                    validation_set_year_distribution.append(year)\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "    validation_eye_dict[\"eye_list\"] = validation_eye_list\n",
    "    validation_eye_dict[\"label_list\"] = validation_label_list\n",
    "    \n",
    "    # test set\n",
    "    unique_year = count_unique_year(test_set)\n",
    "    \n",
    "    for y in unique_year:\n",
    "        test_eye_dict[y] = dict()\n",
    "        test_eye_dict[y][\"eye_list\"] = []\n",
    "        test_eye_dict[y][\"label_list\"] = []\n",
    "\n",
    "    for pid, value in test_set.items():\n",
    "        \n",
    "        re_year = np.array(value[\"re\"][\"re_year\"])\n",
    "        re_img_list = value[\"re\"][\"re_img\"]\n",
    "        re_severe_score = value[\"re\"][\"re_severe_score\"]\n",
    "        re_late_amd = np.array(value[\"re\"][\"re_late_amd\"])\n",
    "        \n",
    "        if len(re_img_list) != len(re_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        le_year = np.array(value[\"le\"][\"le_year\"])\n",
    "        le_img_list = value[\"le\"][\"le_img\"]\n",
    "        le_severe_score = value[\"le\"][\"le_severe_score\"]\n",
    "        le_late_amd = np.array(value[\"le\"][\"le_late_amd\"])\n",
    "        \n",
    "        if len(le_img_list) != len(le_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        if remove_recurrent: # remove recurrent late-AMD labels\n",
    "            \n",
    "            if np.sum(re_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                re_first_late_amd_idx = np.where(re_late_amd == 1)[0][0]\n",
    "                re_year = re_year[:re_first_late_amd_idx+1]\n",
    "                re_img_list = re_img_list[:re_first_late_amd_idx+1]\n",
    "                re_severe_score = re_severe_score[:re_first_late_amd_idx+1]\n",
    "                re_late_amd = re_late_amd[:re_first_late_amd_idx+1]\n",
    "                \n",
    "            if np.sum(le_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                le_first_late_amd_idx = np.where(le_late_amd == 1)[0][0]\n",
    "                le_year = le_year[:le_first_late_amd_idx+1]\n",
    "                le_img_list = le_img_list[:le_first_late_amd_idx+1]\n",
    "                le_severe_score = le_severe_score[:le_first_late_amd_idx+1]\n",
    "                le_late_amd = le_late_amd[:le_first_late_amd_idx+1]\n",
    "                \n",
    "        if len(re_year) >= 2: \n",
    "        \n",
    "            for idx, year in enumerate(re_year):\n",
    "                longitudinal_eye_img_list = re_img_list[:(idx+1)]\n",
    "                label_year = year + timestamp_delta\n",
    "                label_ind1 = np.where(re_year > year, True, False)\n",
    "                label_ind2 = np.where(re_year <= label_year, True, False)\n",
    "                label_ind = label_ind1 * label_ind2\n",
    "            \n",
    "                if np.sum(label_ind) > 0:\n",
    "                    test_eye_dict[year][\"eye_list\"].append(longitudinal_eye_img_list)\n",
    "                    test_eye_dict[year][\"label_list\"].append(int(np.max(re_late_amd[label_ind])))\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        if len(le_year) >= 2: \n",
    "                \n",
    "            for idx, year in enumerate(le_year):\n",
    "                longitudinal_eye_img_list = le_img_list[:(idx+1)]\n",
    "                label_year = year + timestamp_delta\n",
    "                label_ind1 = np.where(le_year > year, True, False)\n",
    "                label_ind2 = np.where(le_year <= label_year, True, False)\n",
    "                label_ind = label_ind1 * label_ind2\n",
    "            \n",
    "                if np.sum(label_ind) > 0:\n",
    "                    test_eye_dict[year][\"eye_list\"].append(longitudinal_eye_img_list)\n",
    "                    test_eye_dict[year][\"label_list\"].append(int(np.max(le_late_amd[label_ind])))\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "    # count the number of data\n",
    "    print(\"the number of data in train set: {}\".format(len(train_eye_dict[\"eye_list\"])))\n",
    "    print(\"the number of data in validation set: {}\".format(len(validation_eye_dict[\"eye_list\"])))\n",
    "    \n",
    "    test_data_count = 0\n",
    "    for year, value in test_eye_dict.items():\n",
    "        test_data_count += len(value[\"eye_list\"])\n",
    "        \n",
    "    print(\"the number of data in test set: {}\".format(test_data_count))\n",
    "    \n",
    "    if return_statistics:\n",
    "        return {\"train_set\" : train_eye_dict, \"validation_set\" : validation_eye_dict, \"test_set\" : test_eye_dict}, train_set_year_distribution, validation_set_year_distribution\n",
    "    else:\n",
    "        return {\"train_set\" : train_eye_dict, \"validation_set\" : validation_eye_dict, \"test_set\" : test_eye_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of data in train set: 33870\n",
      "the number of data in validation set: 7213\n",
      "the number of data in test set: 7264\n"
     ]
    }
   ],
   "source": [
    "longitudinal_eye_data_dict_timedelta2, ts, vs = build_longitudinal_prediction_data_dict(splitted_patient_dict, timestamp_delta=2, remove_recurrent=True, return_statistics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_eye_data_dict_timedelta2.pkl\", longitudinal_eye_data_dict_timedelta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudinal_eye_data_dict_timedelta2 = load_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_eye_data_dict_timedelta2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_label_per_year(set_distribution):\n",
    "    \n",
    "    count_label_dict = dict()\n",
    "    unique_year = np.unique(set_distribution)\n",
    "    \n",
    "    for year in unique_year:\n",
    "        count_label_dict[year] = 0\n",
    "        \n",
    "    for y in set_distribution:\n",
    "        count_label_dict[y] += 1\n",
    "        \n",
    "    return count_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_test_label_per_year(data_dict):\n",
    "    \n",
    "    dataset_dict = data_dict[\"test_set\"]\n",
    "    unique_year = np.array(list(dataset_dict.keys()))\n",
    "    unique_year = np.sort(unique_year)\n",
    "    \n",
    "    late_amd_count_list = []\n",
    "    non_late_amd_count_list = []\n",
    "    \n",
    "    for year in unique_year:\n",
    "        \n",
    "        total_count = len(dataset_dict[year][\"label_list\"])\n",
    "        late_amd_count = np.sum(dataset_dict[year][\"label_list\"])\n",
    "        non_late_amd_count = total_count - late_amd_count\n",
    "        \n",
    "        late_amd_count_list.append(late_amd_count)\n",
    "        non_late_amd_count_list.append(non_late_amd_count)\n",
    "        \n",
    "    print(\"Year /// late-AMD count /// non late-AMD count\")\n",
    "    for year, late_amd_count, non_late_amd_count in zip(unique_year, late_amd_count_list, non_late_amd_count_list):\n",
    "        print(\"Year {}: {} /// {}\".format(year, late_amd_count, non_late_amd_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year /// late-AMD count /// non late-AMD count\n",
      "Year 0.0: 41 /// 937\n",
      "Year 0.5: 2 /// 16\n",
      "Year 1.0: 3 /// 37\n",
      "Year 1.5: 5 /// 23\n",
      "Year 2.0: 26 /// 889\n",
      "Year 2.5: 1 /// 25\n",
      "Year 3.0: 29 /// 951\n",
      "Year 3.5: 1 /// 15\n",
      "Year 4.0: 19 /// 804\n",
      "Year 4.5: 1 /// 21\n",
      "Year 5.0: 23 /// 813\n",
      "Year 5.5: 1 /// 36\n",
      "Year 6.0: 15 /// 606\n",
      "Year 6.5: 0 /// 49\n",
      "Year 7.0: 11 /// 548\n",
      "Year 7.5: 0 /// 43\n",
      "Year 8.0: 6 /// 433\n",
      "Year 8.5: 0 /// 3\n",
      "Year 9.0: 5 /// 457\n",
      "Year 9.5: 1 /// 1\n",
      "Year 10.0: 5 /// 276\n",
      "Year 11.0: 1 /// 85\n",
      "Year 12.0: 0.0 /// 0.0\n",
      "Year 13.0: 0.0 /// 0.0\n"
     ]
    }
   ],
   "source": [
    "count_label_per_year(longitudinal_eye_data_dict_timedelta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudinal_eye_data_dict_timedelta2 = load_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_eye_data_dict_timedelta2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_n_recent_longitudinal_prediction_data_dict(longitudinal_eye_data_dict, n_recent):\n",
    "    \n",
    "    training_eye_list = longitudinal_eye_data_dict[\"train_set\"][\"eye_list\"]\n",
    "    training_label_list = longitudinal_eye_data_dict[\"train_set\"][\"label_list\"]\n",
    "    validation_eye_list = longitudinal_eye_data_dict[\"validation_set\"][\"eye_list\"]\n",
    "    validation_label_list = longitudinal_eye_data_dict[\"validation_set\"][\"label_list\"]\n",
    "    test_set = longitudinal_eye_data_dict[\"test_set\"]\n",
    "    \n",
    "    n_recent_train_set = dict()\n",
    "    n_recent_validation_set = dict()\n",
    "    n_recent_test_set = dict()\n",
    "    \n",
    "    n_recent_training_eye_list = []\n",
    "    n_recent_training_label_list = []\n",
    "    n_recent_validation_eye_list = []\n",
    "    n_recent_validation_label_list = []\n",
    "    \n",
    "    for eye_list, label in zip(training_eye_list, training_label_list):\n",
    "        n_recent_training_eye_list.append(eye_list[-n_recent:])\n",
    "        n_recent_training_label_list.append(label)\n",
    "        \n",
    "    n_recent_train_set[\"eye_list\"] = n_recent_training_eye_list\n",
    "    n_recent_train_set[\"label_list\"] = n_recent_training_label_list\n",
    "        \n",
    "    for eye_list, label in zip(validation_eye_list, validation_label_list):\n",
    "        n_recent_validation_eye_list.append(eye_list[-n_recent:])\n",
    "        n_recent_validation_label_list.append(label)\n",
    "        \n",
    "    n_recent_validation_set[\"eye_list\"] = n_recent_validation_eye_list\n",
    "    n_recent_validation_set[\"label_list\"] = n_recent_validation_label_list\n",
    "    \n",
    "    unique_year = list(test_set.keys())\n",
    "    \n",
    "    for year in unique_year:\n",
    "        this_year_eye_list = test_set[year][\"eye_list\"]\n",
    "        this_year_label_list = test_set[year][\"label_list\"]\n",
    "        \n",
    "        this_year_n_recent_test_eye_list = []\n",
    "        this_year_n_recent_test_label_list = []\n",
    "        \n",
    "        for eye_list, label in zip(this_year_eye_list, this_year_label_list):\n",
    "            this_year_n_recent_test_eye_list.append(eye_list[-n_recent:])\n",
    "            this_year_n_recent_test_label_list.append(label)\n",
    "            \n",
    "        n_recent_test_set[year] = {\"eye_list\" : this_year_n_recent_test_eye_list, \"label_list\" : this_year_n_recent_test_label_list}\n",
    "        \n",
    "    return {\"train_set\" : n_recent_train_set, \"validation_set\" : n_recent_validation_set, \"test_set\" : n_recent_test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudinal_eye_data_dict_timedelta2_recent3 = build_n_recent_longitudinal_prediction_data_dict(longitudinal_eye_data_dict_timedelta2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_eye_data_dict_timedelta2_recent3.pkl\", longitudinal_eye_data_dict_timedelta2_recent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_maxlen(dataset):\n",
    "    \n",
    "    maxlen = 0\n",
    "    \n",
    "    for elem in dataset:\n",
    "        \n",
    "        if len(elem) > maxlen:\n",
    "            maxlen = len(elem)\n",
    "    \n",
    "    return maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_maxlen(longitudinal_eye_data_dict_timedelta2_recent3[\"validation_set\"][\"eye_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_selective_longitudinal_prediction_data_dict(longitudinal_eye_data_dict, n_recent):\n",
    "    \"\"\"\n",
    "    the first and n recent images\n",
    "    \"\"\"\n",
    "    \n",
    "    training_eye_list = longitudinal_eye_data_dict[\"train_set\"][\"eye_list\"]\n",
    "    training_label_list = longitudinal_eye_data_dict[\"train_set\"][\"label_list\"]\n",
    "    validation_eye_list = longitudinal_eye_data_dict[\"validation_set\"][\"eye_list\"]\n",
    "    validation_label_list = longitudinal_eye_data_dict[\"validation_set\"][\"label_list\"]\n",
    "    test_set = longitudinal_eye_data_dict[\"test_set\"]\n",
    "    \n",
    "    selective_train_set = dict()\n",
    "    selective_validation_set = dict()\n",
    "    selective_test_set = dict()\n",
    "    \n",
    "    selective_training_eye_list = []\n",
    "    selective_training_label_list = []\n",
    "    selective_validation_eye_list = []\n",
    "    selective_validation_label_list = []\n",
    "    \n",
    "    for eye_list, label in zip(training_eye_list, training_label_list):\n",
    "        selective_eye_list = []\n",
    "        \n",
    "        if len(eye_list) <= (1+n_recent):\n",
    "            selective_eye_list = eye_list\n",
    "        else:\n",
    "            selective_eye_list.append(eye_list[0])\n",
    "            selective_eye_list.extend(eye_list[-n_recent:])\n",
    "        \n",
    "        selective_training_eye_list.append(selective_eye_list)\n",
    "        selective_training_label_list.append(label)\n",
    "        \n",
    "    selective_train_set[\"eye_list\"] = selective_training_eye_list\n",
    "    selective_train_set[\"label_list\"] = selective_training_label_list\n",
    "        \n",
    "    for eye_list, label in zip(validation_eye_list, validation_label_list):\n",
    "        selective_eye_list = []\n",
    "        \n",
    "        if len(eye_list) <= (1+n_recent):\n",
    "            selective_eye_list = eye_list\n",
    "        else:\n",
    "            selective_eye_list.append(eye_list[0])\n",
    "            selective_eye_list.extend(eye_list[-n_recent:])\n",
    "        \n",
    "        selective_validation_eye_list.append(selective_eye_list)\n",
    "        selective_validation_label_list.append(label)\n",
    "        \n",
    "    selective_validation_set[\"eye_list\"] = selective_validation_eye_list\n",
    "    selective_validation_set[\"label_list\"] = selective_validation_label_list\n",
    "    \n",
    "    unique_year = list(test_set.keys())\n",
    "    \n",
    "    for year in unique_year:\n",
    "        this_year_eye_list = test_set[year][\"eye_list\"]\n",
    "        this_year_label_list = test_set[year][\"label_list\"]\n",
    "        \n",
    "        this_year_selective_test_eye_list = []\n",
    "        this_year_selective_test_label_list = []\n",
    "        \n",
    "        for eye_list, label in zip(this_year_eye_list, this_year_label_list):\n",
    "            selective_eye_list = []\n",
    "            \n",
    "            if len(eye_list) <= (1+n_recent):\n",
    "                selective_eye_list = eye_list\n",
    "            else:\n",
    "                selective_eye_list.append(eye_list[0])\n",
    "                selective_eye_list.extend(eye_list[-n_recent:])\n",
    "                \n",
    "            this_year_selective_test_eye_list.append(selective_eye_list)\n",
    "            this_year_selective_test_label_list.append(label)\n",
    "        \n",
    "            \n",
    "        selective_test_set[year] = {\"eye_list\" : this_year_selective_test_eye_list, \"label_list\" : this_year_selective_test_label_list}\n",
    "        \n",
    "    return {\"train_set\" : selective_train_set, \"validation_set\" : selective_validation_set, \"test_set\" : selective_test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudinal_eye_data_dict_timedelta2_selective = build_selective_longitudinal_prediction_data_dict(longitudinal_eye_data_dict_timedelta2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_eye_data_dict_timedelta2_selective.pkl\", longitudinal_eye_data_dict_timedelta2_selective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longitudinal_eye_data_dict_timedelta2_selective[\"train_set\"][\"label_list\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-workspace",
   "language": "python",
   "name": "python3-workspace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
