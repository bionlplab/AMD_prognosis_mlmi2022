{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only using CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(output_path, mydata):\n",
    "    with open(output_path, 'wb') as f:\n",
    "        \n",
    "        pickle.dump(mydata, f)\n",
    "        \n",
    "def load_data(data_path):\n",
    "    data = pickle.load(open(data_path, 'rb'))\n",
    "\n",
    "    return data\n",
    "\n",
    "def convert_binary_score(severe_score):\n",
    "    \n",
    "    if severe_score >= 10:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def lower_filepath(filepath):\n",
    "    \n",
    "    filename, fileformat = filepath.split(\".\")\n",
    "    fileformat = fileformat.lower()\n",
    "    \n",
    "    converted_filepath = filename + \".\" + fileformat\n",
    "    \n",
    "    return converted_filepath\n",
    "    \n",
    "def shuffle_data(mydata):\n",
    "    mydata = np.array(mydata)\n",
    "    idx = np.arange(len(mydata))\n",
    "    random.shuffle(idx)\n",
    "    \n",
    "    return mydata[idx]\n",
    "    \n",
    "def filter_available_visits(amd_record_visit):\n",
    "    \n",
    "    filtered_visits = []\n",
    "    \n",
    "    for visit in amd_record_visit:\n",
    "        re_label_test = False\n",
    "        re_data_test = False\n",
    "        le_label_test = False\n",
    "        le_data_test = False\n",
    "        \n",
    "        try:\n",
    "            this_re_severe_score = visit[\"AMDSEVRE\"]\n",
    "            if np.isnan(this_re_severe_score):\n",
    "                re_label_test = False\n",
    "            else:\n",
    "                re_label_test = True\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            this_le_severe_score = visit[\"AMDSEVLE\"]\n",
    "            if np.isnan(this_le_severe_score):\n",
    "                le_label_test = False\n",
    "            else:\n",
    "                le_label_test = True\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            this_re_data = visit[\"RE_IMG\"]\n",
    "            if len(this_re_data) > 0:\n",
    "                re_data_test = True\n",
    "            else:\n",
    "                re_data_test = False\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            this_le_data = visit[\"LE_IMG\"]\n",
    "            if len(this_le_data) > 0:\n",
    "                le_data_test = True\n",
    "            else:\n",
    "                le_data_test = False\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        test_result = re_label_test * le_label_test * re_data_test * le_data_test\n",
    "        \n",
    "        if test_result == 1:\n",
    "            filtered_visits.append(visit)\n",
    "            \n",
    "    return filtered_visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_patient_dict(data_dir, remove_recurrent=True):\n",
    "    \n",
    "    json_data = open(data_dir)\n",
    "    amd_data = json.load(json_data)\n",
    "    patient_dict = dict()\n",
    "    \n",
    "    count_idx = 0\n",
    "    count_patient = 0\n",
    "    count_removed_patient = 0\n",
    "    for idx, record in enumerate(amd_data):\n",
    "        count_patient += 1\n",
    "        if idx % 100 == 0:\n",
    "            print(\"{} patients processed...\".format(count_idx*100))\n",
    "            count_idx += 1\n",
    "        \n",
    "        this_id = record[\"ID2\"]\n",
    "        this_visits = record[\"VISITS\"]\n",
    "        filtered_visits = filter_available_visits(this_visits)\n",
    "        this_re = dict()\n",
    "        this_le = dict()\n",
    "        \n",
    "        re_year = []\n",
    "        le_year = []\n",
    "        re_img = []\n",
    "        le_img = []\n",
    "        re_severe_score = []\n",
    "        le_severe_score = []\n",
    "        re_late_amd = []\n",
    "        le_late_amd = []\n",
    "        \n",
    "        if len(filtered_visits) <= 1:\n",
    "            count_removed_patient += 1\n",
    "            continue\n",
    "            \n",
    "        for i, visit in enumerate(filtered_visits):\n",
    "            \n",
    "            re_year.append(int(visit[\"VISNO\"])/2)\n",
    "            le_year.append(int(visit[\"VISNO\"])/2)\n",
    "            re_img.append(lower_filepath(visit[\"RE_IMG\"]))\n",
    "            le_img.append(lower_filepath(visit[\"LE_IMG\"]))\n",
    "            re_severe_score.append(visit[\"AMDSEVRE\"])\n",
    "            le_severe_score.append(visit[\"AMDSEVLE\"])\n",
    "            re_late_amd.append(convert_binary_score(visit[\"AMDSEVRE\"]))\n",
    "            le_late_amd.append(convert_binary_score(visit[\"AMDSEVLE\"]))\n",
    "            \n",
    "        re_late_amd = np.array(re_late_amd)\n",
    "        le_late_amd = np.array(le_late_amd)\n",
    "        \n",
    "        this_re[\"re_year\"] = re_year\n",
    "        this_re[\"re_img\"] = re_img\n",
    "        this_re[\"re_severe_score\"] = re_severe_score\n",
    "        this_re[\"re_late_amd\"] = re_late_amd\n",
    "        \n",
    "        this_le[\"le_year\"] = le_year\n",
    "        this_le[\"le_img\"] = le_img\n",
    "        this_le[\"le_severe_score\"] = le_severe_score\n",
    "        this_le[\"le_late_amd\"] = le_late_amd\n",
    "        \n",
    "        patient_late_amd_check = np.sum(re_late_amd) + np.sum(le_late_amd)\n",
    "        \n",
    "        if patient_late_amd_check > 0:\n",
    "            patient_late_amd_label = 1\n",
    "        else:\n",
    "            patient_late_amd_label = 0\n",
    "        \n",
    "        patient_dict[this_id] = {\"re\" : this_re, \"le\" : this_le, \"late_amd_label\" : patient_late_amd_label}\n",
    "    \n",
    "    print(\"the number of patients: {}\".format(count_patient))\n",
    "    print(\"{} patients that have no valid visits were excluded\".format(count_removed_patient))\n",
    "    \n",
    "    return patient_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 patients processed...\n",
      "100 patients processed...\n",
      "200 patients processed...\n",
      "300 patients processed...\n",
      "400 patients processed...\n",
      "500 patients processed...\n",
      "600 patients processed...\n",
      "700 patients processed...\n",
      "800 patients processed...\n",
      "900 patients processed...\n",
      "1000 patients processed...\n",
      "1100 patients processed...\n",
      "1200 patients processed...\n",
      "1300 patients processed...\n",
      "1400 patients processed...\n",
      "1500 patients processed...\n",
      "1600 patients processed...\n",
      "1700 patients processed...\n",
      "1800 patients processed...\n",
      "1900 patients processed...\n",
      "2000 patients processed...\n",
      "2100 patients processed...\n",
      "2200 patients processed...\n",
      "2300 patients processed...\n",
      "2400 patients processed...\n",
      "2500 patients processed...\n",
      "2600 patients processed...\n",
      "2700 patients processed...\n",
      "2800 patients processed...\n",
      "2900 patients processed...\n",
      "3000 patients processed...\n",
      "3100 patients processed...\n",
      "3200 patients processed...\n",
      "3300 patients processed...\n",
      "3400 patients processed...\n",
      "3500 patients processed...\n",
      "3600 patients processed...\n",
      "3700 patients processed...\n",
      "3800 patients processed...\n",
      "3900 patients processed...\n",
      "4000 patients processed...\n",
      "4100 patients processed...\n",
      "4200 patients processed...\n",
      "4300 patients processed...\n",
      "4400 patients processed...\n",
      "4500 patients processed...\n",
      "4600 patients processed...\n",
      "4700 patients processed...\n",
      "the number of patients: 4757\n",
      "442 patients that have no valid visits were excluded\n"
     ]
    }
   ],
   "source": [
    "patient_dict = build_patient_dict(\"/home/jl5307/current_research/AMD_prediction/data/AREDS_participants_amd3.json\", remove_recurrent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'re': {'re_year': [0.0, 2.0, 3.0, 4.0, 5.0, 7.0, 8.0, 10.0],\n",
       "  're_img': ['51685 QUA F2 RE LS.jpg',\n",
       "   '51685 04 F2 RE LS.jpg',\n",
       "   '51685 06 F2 RE LS.jpg',\n",
       "   '51685 08 F2 RE LS.jpg',\n",
       "   '51685 10 F2 RE LS.jpg',\n",
       "   '51685 14 F2 RE LS.jpg',\n",
       "   '51685 16 F2 RE LS.jpg',\n",
       "   '51685 20 F2 RE LS.jpg'],\n",
       "  're_severe_score': [4.0, 3.0, 4.0, 4.0, 5.0, 6.0, 8.0, 7.0],\n",
       "  're_late_amd': array([0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " 'le': {'le_year': [0.0, 2.0, 3.0, 4.0, 5.0, 7.0, 8.0, 10.0],\n",
       "  'le_img': ['51685 QUA F2 LE LS.jpg',\n",
       "   '51685 04 F2 LE LS.jpg',\n",
       "   '51685 06 F2 LE LS.jpg',\n",
       "   '51685 08 F2 LE LS.jpg',\n",
       "   '51685 10 F2 LE LS.jpg',\n",
       "   '51685 14 F2 LE LS.jpg',\n",
       "   '51685 16 F2 LE LS.jpg',\n",
       "   '51685 20 F2 LE LS.jpg'],\n",
       "  'le_severe_score': [1.0, 3.0, 3.0, 2.0, 4.0, 6.0, 8.0, 6.0],\n",
       "  'le_late_amd': array([0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " 'late_amd_label': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_dict[\"1001\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split patient dict into train-validation-test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_patient_dict(patient_dict, train_size, validation_size, test_size):\n",
    "    \n",
    "    late_amd_patient_list = []\n",
    "    non_late_amd_patient_list = []\n",
    "    \n",
    "    for pid, value in patient_dict.items():\n",
    "        \n",
    "        if value[\"late_amd_label\"] == 1:\n",
    "            late_amd_patient_list.append(pid)\n",
    "        else:\n",
    "            non_late_amd_patient_list.append(pid)\n",
    "            \n",
    "    late_amd_patient_list = shuffle_data(late_amd_patient_list)\n",
    "    non_late_amd_patient_list = shuffle_data(non_late_amd_patient_list)\n",
    "    \n",
    "    print(\"the number of patients have late-AMD at least one eye: {}\".format(len(late_amd_patient_list)))\n",
    "    print(\"the number of patients do have late-AMD at least one eye: {}\".format(len(non_late_amd_patient_list)))\n",
    "    \n",
    "    ## train set\n",
    "    train_late_amd_patient_list = late_amd_patient_list[:int(np.floor(len(late_amd_patient_list)*train_size))]\n",
    "    train_non_late_amd_patient_list = non_late_amd_patient_list[:int(np.floor(len(non_late_amd_patient_list)*train_size))]  \n",
    "    \n",
    "    ## test and validation set\n",
    "    test_validation_late_amd_patient_list = late_amd_patient_list[int(np.floor(len(late_amd_patient_list)*train_size)):]\n",
    "    test_validation_non_late_amd_patient_list = non_late_amd_patient_list[int(np.floor(len(non_late_amd_patient_list)*train_size)):]\n",
    "    \n",
    "    late_amd_test_validation_slicing_ind = int(np.floor(len(test_validation_late_amd_patient_list) * (test_size / (validation_size+test_size))))\n",
    "    non_late_amd_test_validation_slicing_ind = int(np.floor(len(test_validation_non_late_amd_patient_list) * (test_size / (validation_size+test_size))))\n",
    "    \n",
    "    test_late_amd_patient_list = test_validation_late_amd_patient_list[:late_amd_test_validation_slicing_ind]\n",
    "    test_non_late_amd_patient_list = test_validation_non_late_amd_patient_list[:non_late_amd_test_validation_slicing_ind]\n",
    "    \n",
    "    validation_late_amd_patient_list = test_validation_late_amd_patient_list[late_amd_test_validation_slicing_ind:]\n",
    "    validation_non_late_amd_patient_list = test_validation_non_late_amd_patient_list[non_late_amd_test_validation_slicing_ind:]\n",
    "\n",
    "    train_pid_list = []\n",
    "    validation_pid_list = []\n",
    "    test_pid_list = []\n",
    "\n",
    "    train_pid_list.extend(train_late_amd_patient_list)\n",
    "    train_pid_list.extend(train_non_late_amd_patient_list)\n",
    "    validation_pid_list.extend(validation_late_amd_patient_list)\n",
    "    validation_pid_list.extend(validation_non_late_amd_patient_list)\n",
    "    test_pid_list.extend(test_late_amd_patient_list)\n",
    "    test_pid_list.extend(test_non_late_amd_patient_list)\n",
    "    \n",
    "    train_pid_list = shuffle_data(train_pid_list)\n",
    "    validation_pid_list = shuffle_data(validation_pid_list)\n",
    "    test_pid_list = shuffle_data(test_pid_list)\n",
    "\n",
    "    train_set = dict()\n",
    "    validation_set = dict()\n",
    "    test_set = dict()\n",
    "    \n",
    "    for pid in train_pid_list:\n",
    "        train_set[pid] = patient_dict[pid]\n",
    "    \n",
    "    for pid in validation_pid_list:\n",
    "        validation_set[pid] = patient_dict[pid]\n",
    "        \n",
    "    for pid in test_pid_list:\n",
    "        test_set[pid] = patient_dict[pid]\n",
    "        \n",
    "    print(len(train_set))\n",
    "    print(len(validation_set))\n",
    "    print(len(test_set))\n",
    "    \n",
    "    return {\"train_set\" : train_set, \"validation_set\" : validation_set, \"test_set\" : test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of patients have late-AMD at least one eye: 1223\n",
      "the number of patients do have late-AMD at least one eye: 3092\n",
      "3020\n",
      "648\n",
      "647\n"
     ]
    }
   ],
   "source": [
    "splitted_patient_dict = split_patient_dict(patient_dict, 0.7, 0.15, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/splitted_patient_dict.pkl\", splitted_patient_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_patient_dict = load_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/splitted_patient_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_length(eye_list, label_list, min_len):\n",
    "    \n",
    "    filtered_eye_list = []\n",
    "    filtered_label_list = []\n",
    "    \n",
    "    for eye_img_list, eye_label_list in zip(eye_list, label_list):\n",
    "        \n",
    "        if len(eye_img_list) >= min_len and len(eye_label_list) >= min_len:\n",
    "            filtered_eye_list.append(eye_img_list)\n",
    "            filtered_label_list.append(eye_label_list)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return filtered_eye_list, filtered_label_list\n",
    "\n",
    "def build_longitudinal_sequential_prediction_data_dict(splitted_patient_dict, timedelta, min_length, remove_recurrent=True, generate_per_len_test_set=True):\n",
    "    \n",
    "    train_set = splitted_patient_dict[\"train_set\"]\n",
    "    validation_set = splitted_patient_dict[\"validation_set\"]\n",
    "    test_set = splitted_patient_dict[\"test_set\"]\n",
    "    \n",
    "    train_eye_dict = dict()\n",
    "    validation_eye_dict = dict()\n",
    "    test_eye_dict = dict()\n",
    "    \n",
    "    train_eye_list = []\n",
    "    train_label_list = []\n",
    "    validation_eye_list = []\n",
    "    validation_label_list = []\n",
    "    test_eye_list = []\n",
    "    test_label_list = []\n",
    "    \n",
    "    train_eye_exclusion_count = 0\n",
    "    validation_eye_exclusion_count = 0\n",
    "    test_eye_exclusion_count = 0\n",
    "    \n",
    "    # train set\n",
    "    for pid, value in train_set.items():\n",
    "        \n",
    "        re_year = np.array(value[\"re\"][\"re_year\"])\n",
    "        re_img_list = value[\"re\"][\"re_img\"]\n",
    "        re_severe_score = value[\"re\"][\"re_severe_score\"]\n",
    "        re_late_amd = np.array(value[\"re\"][\"re_late_amd\"])\n",
    "        \n",
    "        if len(re_img_list) != len(re_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        le_year = np.array(value[\"le\"][\"le_year\"])\n",
    "        le_img_list = value[\"le\"][\"le_img\"]\n",
    "        le_severe_score = value[\"le\"][\"le_severe_score\"]\n",
    "        le_late_amd = np.array(value[\"le\"][\"le_late_amd\"])\n",
    "        \n",
    "        if len(le_img_list) != len(le_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "            \n",
    "        if remove_recurrent:\n",
    "            \n",
    "            if np.sum(re_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                re_first_late_amd_idx = np.where(re_late_amd == 1)[0][0]\n",
    "                re_year = re_year[:re_first_late_amd_idx+1]\n",
    "                re_img_list = re_img_list[:re_first_late_amd_idx+1]\n",
    "                re_severe_score = re_severe_score[:re_first_late_amd_idx+1]\n",
    "                re_late_amd = re_late_amd[:re_first_late_amd_idx+1]\n",
    "                \n",
    "            if np.sum(le_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                le_first_late_amd_idx = np.where(le_late_amd == 1)[0][0]\n",
    "                le_year = le_year[:le_first_late_amd_idx+1]\n",
    "                le_img_list = le_img_list[:le_first_late_amd_idx+1]\n",
    "                le_severe_score = le_severe_score[:le_first_late_amd_idx+1]\n",
    "                le_late_amd = le_late_amd[:le_first_late_amd_idx+1]\n",
    "            \n",
    "        re_total_year_len = re_year[-1] - re_year[0]\n",
    "        le_total_year_len = le_year[-1] - le_year[0]\n",
    "        re_label_list = []\n",
    "        le_label_list = []\n",
    "        \n",
    "        if re_total_year_len >= timedelta:\n",
    "            for idx, (year, img) in enumerate(zip(re_year, re_img_list)):\n",
    "                \n",
    "                if idx == (len(re_year)-1):\n",
    "                    continue\n",
    "                else:\n",
    "                    label_year_end = year + timedelta\n",
    "                    label_year_ind1 = np.where(re_year > year, True, False)\n",
    "                    label_year_ind2 = np.where(re_year <= label_year_end, True, False)\n",
    "                    label_ind = label_year_ind1 * label_year_ind2\n",
    "                \n",
    "                    if np.sum(label_ind) > 0:\n",
    "                        re_label_list.append(int(np.max(re_late_amd[label_ind])))\n",
    "                    else:\n",
    "                        re_label_list.append(re_late_amd[idx+1])\n",
    "            \n",
    "        else:\n",
    "            train_eye_exclusion_count += 1\n",
    "            \n",
    "        if le_total_year_len >= timedelta:\n",
    "            for idx, (year, img) in enumerate(zip(le_year, le_img_list)):\n",
    "                \n",
    "                if idx == (len(le_year)-1):\n",
    "                    continue\n",
    "                else:\n",
    "                    label_year_end = year + timedelta\n",
    "                    label_year_ind1 = np.where(le_year > year, True, False)\n",
    "                    label_year_ind2 = np.where(le_year <= label_year_end, True, False)\n",
    "                    label_ind = label_year_ind1 * label_year_ind2\n",
    "                \n",
    "                    if np.sum(label_ind) > 0:\n",
    "                        le_label_list.append(int(np.max(le_late_amd[label_ind])))\n",
    "                    else:\n",
    "                        le_label_list.append(le_late_amd[idx+1])\n",
    "            \n",
    "        else:\n",
    "            train_eye_exclusion_count += 1\n",
    "            \n",
    "        if len(re_label_list) > 0:\n",
    "            assert len(re_img_list[:-1]) == len(re_label_list), \"length of the label list and img list must be the same\"\n",
    "            train_eye_list.append(re_img_list[:-1])\n",
    "            train_label_list.append(re_label_list)\n",
    "        \n",
    "        if len(le_label_list) > 0:\n",
    "            assert len(le_img_list[:-1]) == len(le_label_list), \"length of the label list and img list must be the same\"\n",
    "            train_eye_list.append(le_img_list[:-1])\n",
    "            train_label_list.append(le_label_list)\n",
    "        \n",
    "    train_eye_dict[\"eye_list\"] = train_eye_list\n",
    "    train_eye_dict[\"label_list\"] = train_label_list\n",
    "    \n",
    "    # validation set\n",
    "    for pid, value in validation_set.items():\n",
    "        \n",
    "        re_year = np.array(value[\"re\"][\"re_year\"])\n",
    "        re_img_list = value[\"re\"][\"re_img\"]\n",
    "        re_severe_score = value[\"re\"][\"re_severe_score\"]\n",
    "        re_late_amd = np.array(value[\"re\"][\"re_late_amd\"])\n",
    "        \n",
    "        if len(re_img_list) != len(re_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        le_year = np.array(value[\"le\"][\"le_year\"])\n",
    "        le_img_list = value[\"le\"][\"le_img\"]\n",
    "        le_severe_score = value[\"le\"][\"le_severe_score\"]\n",
    "        le_late_amd = np.array(value[\"le\"][\"le_late_amd\"])\n",
    "        \n",
    "        if len(le_img_list) != len(le_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "            \n",
    "        if remove_recurrent:\n",
    "            \n",
    "            if np.sum(re_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                re_first_late_amd_idx = np.where(re_late_amd == 1)[0][0]\n",
    "                re_year = re_year[:re_first_late_amd_idx+1]\n",
    "                re_img_list = re_img_list[:re_first_late_amd_idx+1]\n",
    "                re_severe_score = re_severe_score[:re_first_late_amd_idx+1]\n",
    "                re_late_amd = re_late_amd[:re_first_late_amd_idx+1]\n",
    "                \n",
    "            if np.sum(le_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                le_first_late_amd_idx = np.where(le_late_amd == 1)[0][0]\n",
    "                le_year = le_year[:le_first_late_amd_idx+1]\n",
    "                le_img_list = le_img_list[:le_first_late_amd_idx+1]\n",
    "                le_severe_score = le_severe_score[:le_first_late_amd_idx+1]\n",
    "                le_late_amd = le_late_amd[:le_first_late_amd_idx+1]\n",
    "            \n",
    "        re_total_year_len = re_year[-1] - re_year[0]\n",
    "        le_total_year_len = le_year[-1] - le_year[0]\n",
    "        re_label_list = []\n",
    "        le_label_list = []\n",
    "        \n",
    "        if re_total_year_len >= timedelta:\n",
    "            for idx, (year, img) in enumerate(zip(re_year, re_img_list)):\n",
    "                \n",
    "                if idx == (len(re_year)-1):\n",
    "                    continue\n",
    "                else:\n",
    "                    label_year_end = year + timedelta\n",
    "                    label_year_ind1 = np.where(re_year > year, True, False)\n",
    "                    label_year_ind2 = np.where(re_year <= label_year_end, True, False)\n",
    "                    label_ind = label_year_ind1 * label_year_ind2\n",
    "                \n",
    "                    if np.sum(label_ind) > 0:\n",
    "                        re_label_list.append(int(np.max(re_late_amd[label_ind])))\n",
    "                    else:\n",
    "                        re_label_list.append(re_late_amd[idx+1])\n",
    "            \n",
    "        else:\n",
    "            validation_eye_exclusion_count += 1\n",
    "            \n",
    "        if le_total_year_len >= timedelta:\n",
    "            for idx, (year, img) in enumerate(zip(le_year, le_img_list)):\n",
    "                \n",
    "                if idx == (len(le_year)-1):\n",
    "                    continue\n",
    "                else:\n",
    "                    label_year_end = year + timedelta\n",
    "                    label_year_ind1 = np.where(le_year > year, True, False)\n",
    "                    label_year_ind2 = np.where(le_year <= label_year_end, True, False)\n",
    "                    label_ind = label_year_ind1 * label_year_ind2\n",
    "                \n",
    "                    if np.sum(label_ind) > 0:\n",
    "                        le_label_list.append(int(np.max(le_late_amd[label_ind])))\n",
    "                    else:\n",
    "                        le_label_list.append(le_late_amd[idx+1])\n",
    "            \n",
    "        else:\n",
    "            validation_eye_exclusion_count += 1\n",
    "            \n",
    "        if len(re_label_list) > 0:\n",
    "            assert len(re_img_list[:-1]) == len(re_label_list), \"length of the label list and img list must be the same\"\n",
    "            validation_eye_list.append(re_img_list[:-1])\n",
    "            validation_label_list.append(re_label_list)\n",
    "        \n",
    "        if len(le_label_list) > 0:\n",
    "            assert len(le_img_list[:-1]) == len(le_label_list), \"length of the label list and img list must be the same\"\n",
    "            validation_eye_list.append(le_img_list[:-1])\n",
    "            validation_label_list.append(le_label_list)\n",
    "        \n",
    "    validation_eye_dict[\"eye_list\"] = validation_eye_list\n",
    "    validation_eye_dict[\"label_list\"] = validation_label_list\n",
    "    \n",
    "    # test set\n",
    "    for pid, value in test_set.items():\n",
    "        \n",
    "        re_year = np.array(value[\"re\"][\"re_year\"])\n",
    "        re_img_list = value[\"re\"][\"re_img\"]\n",
    "        re_severe_score = value[\"re\"][\"re_severe_score\"]\n",
    "        re_late_amd = np.array(value[\"re\"][\"re_late_amd\"])\n",
    "        \n",
    "        if len(re_img_list) != len(re_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "        \n",
    "        le_year = np.array(value[\"le\"][\"le_year\"])\n",
    "        le_img_list = value[\"le\"][\"le_img\"]\n",
    "        le_severe_score = value[\"le\"][\"le_severe_score\"]\n",
    "        le_late_amd = np.array(value[\"le\"][\"le_late_amd\"])\n",
    "        \n",
    "        if len(le_img_list) != len(le_late_amd):\n",
    "            raise ValueError(\"the length of img_list and label_list must be the same\")\n",
    "            \n",
    "        if remove_recurrent:\n",
    "            \n",
    "            if np.sum(re_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                re_first_late_amd_idx = np.where(re_late_amd == 1)[0][0]\n",
    "                re_year = re_year[:re_first_late_amd_idx+1]\n",
    "                re_img_list = re_img_list[:re_first_late_amd_idx+1]\n",
    "                re_severe_score = re_severe_score[:re_first_late_amd_idx+1]\n",
    "                re_late_amd = re_late_amd[:re_first_late_amd_idx+1]\n",
    "                \n",
    "            if np.sum(le_late_amd) > 0: # test whether the eye had late-amd status\n",
    "                le_first_late_amd_idx = np.where(le_late_amd == 1)[0][0]\n",
    "                le_year = le_year[:le_first_late_amd_idx+1]\n",
    "                le_img_list = le_img_list[:le_first_late_amd_idx+1]\n",
    "                le_severe_score = le_severe_score[:le_first_late_amd_idx+1]\n",
    "                le_late_amd = le_late_amd[:le_first_late_amd_idx+1]\n",
    "            \n",
    "        re_total_year_len = re_year[-1] - re_year[0]\n",
    "        le_total_year_len = le_year[-1] - le_year[0]\n",
    "        re_label_list = []\n",
    "        le_label_list = []\n",
    "        \n",
    "        if re_total_year_len >= timedelta:\n",
    "            for idx, (year, img) in enumerate(zip(re_year, re_img_list)):\n",
    "                \n",
    "                if idx == (len(re_year)-1):\n",
    "                    continue\n",
    "                else:\n",
    "                    label_year_end = year + timedelta\n",
    "                    label_year_ind1 = np.where(re_year > year, True, False)\n",
    "                    label_year_ind2 = np.where(re_year <= label_year_end, True, False)\n",
    "                    label_ind = label_year_ind1 * label_year_ind2\n",
    "                \n",
    "                    if np.sum(label_ind) > 0:\n",
    "                        re_label_list.append(int(np.max(re_late_amd[label_ind])))\n",
    "                    else:\n",
    "                        re_label_list.append(re_late_amd[idx+1])\n",
    "            \n",
    "        else:\n",
    "            test_eye_exclusion_count += 1\n",
    "            \n",
    "        if le_total_year_len >= timedelta:\n",
    "            for idx, (year, img) in enumerate(zip(le_year, le_img_list)):\n",
    "                \n",
    "                if idx == (len(le_year)-1):\n",
    "                    continue\n",
    "                else:\n",
    "                    label_year_end = year + timedelta\n",
    "                    label_year_ind1 = np.where(le_year > year, True, False)\n",
    "                    label_year_ind2 = np.where(le_year <= label_year_end, True, False)\n",
    "                    label_ind = label_year_ind1 * label_year_ind2\n",
    "                \n",
    "                    if np.sum(label_ind) > 0:\n",
    "                        le_label_list.append(int(np.max(le_late_amd[label_ind])))\n",
    "                    else:\n",
    "                        le_label_list.append(le_late_amd[idx+1])\n",
    "            \n",
    "        else:\n",
    "            test_eye_exclusion_count += 1\n",
    "            \n",
    "        if len(re_label_list) > 0:\n",
    "            assert len(re_img_list[:-1]) == len(re_label_list), \"length of the label list and img list must be the same\"\n",
    "            test_eye_list.append(re_img_list[:-1])\n",
    "            test_label_list.append(re_label_list)\n",
    "        \n",
    "        if len(le_label_list) > 0:\n",
    "            assert len(le_img_list[:-1]) == len(le_label_list), \"length of the label list and img list must be the same\"\n",
    "            test_eye_list.append(le_img_list[:-1])\n",
    "            test_label_list.append(le_label_list)\n",
    "        \n",
    "    test_eye_dict[\"eye_list\"] = test_eye_list\n",
    "    test_eye_dict[\"label_list\"] = test_label_list\n",
    "    \n",
    "    print(\"{} eyes excluded from train set\".format(train_eye_exclusion_count))\n",
    "    print(\"{} eyes excluded from validation set\".format(validation_eye_exclusion_count))\n",
    "    print(\"{} eyes excluded from test set\".format(test_eye_exclusion_count))\n",
    "    \n",
    "    if generate_per_len_test_set:\n",
    "        \n",
    "        per_len_test_eye_dict = dict()\n",
    "\n",
    "        for length in range(min_length):\n",
    "            per_len_test_eye_dict[length+1] = {\"eye_list\" : [], \"label_list\" : []}\n",
    "            \n",
    "        print(\"filter the test data based on the minimum length ({})\".format(min_length))\n",
    "        filtered_test_eye_list, filtered_test_label_list = filter_by_length(test_eye_list, test_label_list, min_length)\n",
    "        assert len(filtered_test_eye_list) == len(filtered_test_label_list), \"the number of eyes and labels must be the same\"\n",
    "        print(\"the number of eye-label pair: {}\".format(len(filtered_test_eye_list)))\n",
    "        \n",
    "        for eye_img_list, eye_label_list in zip(filtered_test_eye_list, filtered_test_label_list):\n",
    "            \n",
    "            for idx in range(min_length):\n",
    "                per_len_test_eye_dict[idx+1][\"eye_list\"].append(eye_img_list[-(idx+1):])\n",
    "                per_len_test_eye_dict[idx+1][\"label_list\"].append(eye_label_list[-(idx+1):])\n",
    "\n",
    "        return {\"train_set\" : train_eye_dict, \"validation_set\" : validation_eye_dict, \"test_set\" : test_eye_dict, \"per_length_test_set\" : per_len_test_eye_dict}\n",
    "    \n",
    "    else:\n",
    "        return {\"train_set\" : train_eye_dict, \"validation_set\" : validation_eye_dict, \"test_set\" : test_eye_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691 eyes excluded from train set\n",
      "136 eyes excluded from validation set\n",
      "142 eyes excluded from test set\n",
      "filter the test data based on the minimum length (5)\n",
      "the number of eye-label pair: 766\n"
     ]
    }
   ],
   "source": [
    "longitudinal_sequential_prediction_td2_min5_data_dict = build_longitudinal_sequential_prediction_data_dict(splitted_patient_dict, timedelta=2, min_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1670 eyes excluded from train set\n",
      "360 eyes excluded from validation set\n",
      "342 eyes excluded from test set\n",
      "filter the test data based on the minimum length (5)\n",
      "the number of eye-label pair: 763\n"
     ]
    }
   ],
   "source": [
    "longitudinal_sequential_prediction_td5_min5_data_dict = build_longitudinal_sequential_prediction_data_dict(splitted_patient_dict, timedelta=5, min_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_sequential_prediction_td2_min5_data_dict.pkl\",\n",
    "         longitudinal_sequential_prediction_td2_min5_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_sequential_prediction_td5_min5_data_dict.pkl\",\n",
    "         longitudinal_sequential_prediction_td5_min5_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unroll patients in the data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_sequence(sequence):\n",
    "    \n",
    "    unrolled_sequence = []\n",
    "    \n",
    "    for i in range(len(sequence)):\n",
    "        unrolled_sequence.append(sequence[:(i+1)])\n",
    "        \n",
    "    return unrolled_sequence\n",
    "\n",
    "def build_unrolled_longitudinal_sequential_prediction_data_dict(longitudinal_sequential_prediction_data_dict):\n",
    "    # unroll patients' visit sequences in the training, validation, and test set\n",
    "    \n",
    "    train_eye_list = longitudinal_sequential_prediction_data_dict[\"train_set\"][\"eye_list\"]\n",
    "    train_label_list = longitudinal_sequential_prediction_data_dict[\"train_set\"][\"label_list\"]\n",
    "    validation_eye_list = longitudinal_sequential_prediction_data_dict[\"validation_set\"][\"eye_list\"]\n",
    "    validation_label_list = longitudinal_sequential_prediction_data_dict[\"validation_set\"][\"label_list\"]\n",
    "    test_eye_list = longitudinal_sequential_prediction_data_dict[\"test_set\"][\"eye_list\"]\n",
    "    test_label_list = longitudinal_sequential_prediction_data_dict[\"test_set\"][\"label_list\"]\n",
    "    \n",
    "    unrolled_train_eye_list = []\n",
    "    unrolled_train_label_list = []\n",
    "    unrolled_validation_eye_list = []\n",
    "    unrolled_validation_label_list = []\n",
    "    unrolled_test_eye_list = []\n",
    "    unrolled_test_label_list = []\n",
    "    \n",
    "    for eye_list, label_list in zip(train_eye_list, train_label_list):\n",
    "        unrolled_train_eye_list.extend(unroll_sequence(eye_list))\n",
    "        unrolled_train_label_list.extend(unroll_sequence(label_list))\n",
    "\n",
    "    for eye_list, label_list in zip(validation_eye_list, validation_label_list):\n",
    "        unrolled_validation_eye_list.extend(unroll_sequence(eye_list))\n",
    "        unrolled_validation_label_list.extend(unroll_sequence(label_list))\n",
    "        \n",
    "    for eye_list, label_list in zip(test_eye_list, test_label_list):\n",
    "        unrolled_test_eye_list.extend(unroll_sequence(eye_list))\n",
    "        unrolled_test_label_list.extend(unroll_sequence(label_list))\n",
    "        \n",
    "    unrolled_longitudinal_sequential_prediction_data_dict = dict()\n",
    "    \n",
    "    unrolled_longitudinal_sequential_prediction_data_dict[\"train_set\"] = {\"eye_list\" : unrolled_train_eye_list,\n",
    "                                                                          \"label_list\" : unrolled_train_label_list}\n",
    "    unrolled_longitudinal_sequential_prediction_data_dict[\"validation_set\"] = {\"eye_list\" : unrolled_validation_eye_list,\n",
    "                                                                          \"label_list\" : unrolled_validation_label_list}\n",
    "    unrolled_longitudinal_sequential_prediction_data_dict[\"test_set\"] = {\"eye_list\" : unrolled_test_eye_list,\n",
    "                                                                          \"label_list\" : unrolled_test_label_list}\n",
    "    unrolled_longitudinal_sequential_prediction_data_dict[\"per_length_test_set\"] = longitudinal_sequential_prediction_data_dict[\"per_length_test_set\"]\n",
    "    \n",
    "    assert len(unrolled_train_eye_list) == len(unrolled_train_label_list), \"the length of eye list and label list in training set must be the same\"\n",
    "    assert len(unrolled_validation_eye_list) == len(unrolled_validation_label_list), \"the length of eye list and label list in validation set must be the same\"\n",
    "    assert len(unrolled_test_eye_list) == len(unrolled_test_label_list), \"the length of eye list and label list in test set must be the same\"\n",
    "    \n",
    "    print(\"the number of instances in train set: {}\".format(len(unrolled_train_eye_list)))\n",
    "    print(\"the number of instances in validation set: {}\".format(len(unrolled_validation_eye_list)))\n",
    "    print(\"the number of instances in test set: {}\".format(len(unrolled_test_eye_list)))\n",
    "    \n",
    "    return unrolled_longitudinal_sequential_prediction_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of instances in train set: 34536\n",
      "the number of instances in validation set: 7396\n",
      "the number of instances in test set: 7429\n"
     ]
    }
   ],
   "source": [
    "unrolled_longitudinal_sequential_prediction_td2_min5_data_dict = build_unrolled_longitudinal_sequential_prediction_data_dict(longitudinal_sequential_prediction_td2_min5_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of instances in train set: 32627\n",
      "the number of instances in validation set: 6935\n",
      "the number of instances in test set: 6996\n"
     ]
    }
   ],
   "source": [
    "unrolled_longitudinal_sequential_prediction_td5_min5_data_dict = build_unrolled_longitudinal_sequential_prediction_data_dict(longitudinal_sequential_prediction_td5_min5_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_sequential_prediction_td2_min5_data_dict_unrolled.pkl\",\n",
    "         unrolled_longitudinal_sequential_prediction_td2_min5_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_sequential_prediction_td5_min5_data_dict_unrolled.pkl\",\n",
    "         unrolled_longitudinal_sequential_prediction_td5_min5_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data_pair(mydata1, mydata2):\n",
    "    assert len(mydata1) == len(mydata2), \"the length of each data in the data pair must be the same\"\n",
    "    mydata1_array = np.array(mydata1)\n",
    "    mydata2_array = np.array(mydata2)\n",
    "    idx = np.arange(len(mydata1_array))\n",
    "    random.shuffle(idx)\n",
    "    \n",
    "    return list(mydata1_array[idx]), list(mydata2_array[idx])\n",
    "\n",
    "def build_stratified_batch(eye_list, label_list, batch_size):\n",
    "    \n",
    "    late_amd_eye_list = []\n",
    "    non_late_amd_eye_list = []\n",
    "    late_amd_label_list = []\n",
    "    non_late_amd_label_list = []\n",
    "    \n",
    "    for eyes, labels in zip(eye_list, label_list):\n",
    "        \n",
    "        if labels[-1] == 1:\n",
    "            late_amd_eye_list.append(eyes)\n",
    "            late_amd_label_list.append(labels)\n",
    "        else:\n",
    "            non_late_amd_eye_list.append(eyes)\n",
    "            non_late_amd_label_list.append(labels)\n",
    "            \n",
    "    late_amd_eye_list, late_amd_label_list = shuffle_data_pair(late_amd_eye_list, late_amd_label_list)\n",
    "    non_late_amd_eye_list, non_late_amd_label_list = shuffle_data_pair(non_late_amd_eye_list, non_late_amd_label_list)\n",
    "    \n",
    "    non_late_amd_batch = int(np.ceil(len(non_late_amd_eye_list)/(batch_size-1)))\n",
    "    \n",
    "    total_batch = np.max([len(late_amd_eye_list), non_late_amd_batch])\n",
    "    \n",
    "    if total_batch > len(late_amd_eye_list):\n",
    "        addition_length = total_batch - len(late_amd_eye_list)\n",
    "        late_amd_eye_list.extend(late_amd_eye_list[:addition_length])\n",
    "        late_amd_label_list.extend(late_amd_label_list[:addition_length])\n",
    "        \n",
    "    elif total_batch > int(np.ceil(len(non_late_amd_eye_list)/(batch_size-1))):\n",
    "        addition_length = int((total_batch - np.ceil(len(non_late_amd_eye_list)/(batch_size-1))) * 31)\n",
    "        non_late_amd_eye_list.extend(non_late_amd_eye_list[:addition_length])\n",
    "        non_late_amd_label_list.extend(non_late_amd_label_list[:addition_length])\n",
    "    \n",
    "    stratified_eye_list = []\n",
    "    stratified_label_list = []\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        non_late_amd_eyes = non_late_amd_eye_list[(batch_size-1)*i:(batch_size-1)*(i+1)]\n",
    "        non_late_amd_labels = non_late_amd_label_list[(batch_size-1)*i:(batch_size-1)*(i+1)]\n",
    "        late_amd_eyes = late_amd_eye_list[i:(i+1)]\n",
    "        late_amd_labels = late_amd_label_list[i:(i+1)]\n",
    "        \n",
    "        stratified_eye_list.extend(non_late_amd_eyes)\n",
    "        stratified_label_list.extend(non_late_amd_labels)\n",
    "        stratified_eye_list.extend(late_amd_eyes)\n",
    "        stratified_label_list.extend(late_amd_labels)\n",
    "    \n",
    "    return stratified_eye_list, stratified_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_label(dataset):\n",
    "    \n",
    "    label_list = dataset[\"label_list\"]\n",
    "    \n",
    "    total_label_num = 0\n",
    "    total_late_amd = 0\n",
    "    \n",
    "    for eye_label_list in label_list:\n",
    "        total_label_num += len(eye_label_list)\n",
    "        total_late_amd += np.sum(eye_label_list)\n",
    "        \n",
    "    print(\"total label: {}\".format(total_label_num))\n",
    "    print(\"total late amd: {}\".format(total_late_amd))\n",
    "    print(\"total non late amd: {}\".format(total_label_num-total_late_amd))\n",
    "    \n",
    "def count_total_label_unrolled(dataset):\n",
    "    \n",
    "    label_list = dataset[\"label_list\"]\n",
    "    \n",
    "    total_label_num = 0\n",
    "    total_late_amd = 0\n",
    "    \n",
    "    for eye_label_list in label_list:\n",
    "        total_label_num += 1\n",
    "        total_late_amd += np.max(eye_label_list)\n",
    "        \n",
    "    print(\"total label: {}\".format(total_label_num))\n",
    "    print(\"total late amd: {}\".format(total_late_amd))\n",
    "    print(\"total non late amd: {}\".format(total_label_num-total_late_amd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total label: 34536\n",
      "total late amd: 851\n",
      "total non late amd: 33685\n"
     ]
    }
   ],
   "source": [
    "count_total_label_unrolled(unrolled_longitudinal_sequential_prediction_timedelta2_min7_data_dict[\"train_set\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total label: 32627\n",
      "total late amd: 1057\n",
      "total non late amd: 31570\n"
     ]
    }
   ],
   "source": [
    "count_total_label_unrolled(unrolled_longitudinal_sequential_prediction_timedelta5_min7_data_dict[\"train_set\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_per_length_statistics(longitudinal_sequential_prediction_data_dict):\n",
    "    \n",
    "    per_length_statistics = dict()\n",
    "    \n",
    "    per_length_test_set = longitudinal_sequential_prediction_data_dict[\"per_length_test_set\"]\n",
    "    total_late_amd_count = 0\n",
    "    total_patient_count = 0\n",
    "    \n",
    "    for length, length_dict in per_length_test_set.items():\n",
    "        \n",
    "        this_length_label_list = length_dict[\"label_list\"]\n",
    "        total_patient_count += len(this_length_label_list)\n",
    "        this_length_late_amd_count = 0\n",
    "\n",
    "        for label_list in this_length_label_list:\n",
    "            \n",
    "            if label_list[-1] == 1:\n",
    "                this_length_late_amd_count += 1\n",
    "                total_late_amd_count += 1\n",
    "        \n",
    "        per_length_statistics[length] = {\"late_amd_count\" : this_length_late_amd_count, \n",
    "                                         \"non_late_amd_count\" : (len(this_length_label_list) - this_length_late_amd_count)}\n",
    "        \n",
    "    per_length_statistics[\"total\"] = {\"total_late_amd_count\" : total_late_amd_count, \n",
    "                                      \"total_non_late_amd_count\" : total_patient_count - total_late_amd_count}\n",
    "    \n",
    "    return per_length_statistics\n",
    "\n",
    "def calculate_per_length_statistics_unrolled(longitudinal_sequential_prediction_data_dict):\n",
    "    \n",
    "    per_length_statistics = dict()\n",
    "    \n",
    "    per_length_test_set = longitudinal_sequential_prediction_data_dict[\"per_length_test_set\"]\n",
    "    total_late_amd_count = 0\n",
    "    total_data_count = 0\n",
    "    \n",
    "    for length, length_dict in per_length_test_set.items():\n",
    "        \n",
    "        this_length_label_list = length_dict[\"label_list\"]\n",
    "        total_data_count += len(this_length_label_list)\n",
    "        this_length_late_amd_count = 0\n",
    "\n",
    "        for label_list in this_length_label_list:\n",
    "            \n",
    "            if label_list[-1] == 1:\n",
    "                this_length_late_amd_count += 1\n",
    "                total_late_amd_count += 1\n",
    "        \n",
    "        per_length_statistics[length] = {\"late_amd_count\" : this_length_late_amd_count, \n",
    "                                         \"non_late_amd_count\" : (len(this_length_label_list) - this_length_late_amd_count)}\n",
    "        \n",
    "    per_length_statistics[\"total\"] = {\"total_late_amd_count\" : total_late_amd_count, \n",
    "                                      \"total_non_late_amd_count\" : total_data_count - total_late_amd_count}\n",
    "    \n",
    "    return per_length_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_min7 = load_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_sequential_prediction_timedelta2_min7_data_dict_unrolled.pkl\")\n",
    "data_dict_min5 = load_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_sequential_prediction_timedelta2_data_dict_min5_unrolled.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(longitudinal_sequential_prediction_timedelta2_min5_data_dict[\"per_length_test_set\"][2][\"eye_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict_min5[\"per_length_test_set\"][2][\"eye_list\"] == longitudinal_sequential_prediction_timedelta2_min5_data_dict[\"per_length_test_set\"][2][\"eye_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'late_amd_count': 20, 'non_late_amd_count': 567},\n",
       " 2: {'late_amd_count': 20, 'non_late_amd_count': 567},\n",
       " 3: {'late_amd_count': 20, 'non_late_amd_count': 567},\n",
       " 4: {'late_amd_count': 20, 'non_late_amd_count': 567},\n",
       " 5: {'late_amd_count': 20, 'non_late_amd_count': 567},\n",
       " 6: {'late_amd_count': 20, 'non_late_amd_count': 567},\n",
       " 7: {'late_amd_count': 20, 'non_late_amd_count': 567},\n",
       " 'total': {'total_late_amd_count': 140, 'total_non_late_amd_count': 3969}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_per_length_statistics_unrolled(data_dict_min7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'late_amd_count': 38, 'non_late_amd_count': 728},\n",
       " 2: {'late_amd_count': 38, 'non_late_amd_count': 728},\n",
       " 3: {'late_amd_count': 38, 'non_late_amd_count': 728},\n",
       " 4: {'late_amd_count': 38, 'non_late_amd_count': 728},\n",
       " 5: {'late_amd_count': 38, 'non_late_amd_count': 728},\n",
       " 'total': {'total_late_amd_count': 190, 'total_non_late_amd_count': 3640}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_per_length_statistics_unrolled(data_dict_min5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0],\n",
       " [0]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unrolled_longitudinal_sequential_prediction_timedelta2_min7_data_dict[\"per_length_test_set\"][1][\"label_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'late_amd_count': 34, 'non_late_amd_count': 1118},\n",
       " 2: {'late_amd_count': 42, 'non_late_amd_count': 1047},\n",
       " 3: {'late_amd_count': 32, 'non_late_amd_count': 962},\n",
       " 4: {'late_amd_count': 23, 'non_late_amd_count': 856},\n",
       " 5: {'late_amd_count': 18, 'non_late_amd_count': 748},\n",
       " 6: {'late_amd_count': 13, 'non_late_amd_count': 694},\n",
       " 7: {'late_amd_count': 10, 'non_late_amd_count': 577},\n",
       " 8: {'late_amd_count': 7, 'non_late_amd_count': 507},\n",
       " 9: {'late_amd_count': 6, 'non_late_amd_count': 416},\n",
       " 10: {'late_amd_count': 4, 'non_late_amd_count': 228},\n",
       " 11: {'late_amd_count': 3, 'non_late_amd_count': 76},\n",
       " 12: {'late_amd_count': 0, 'non_late_amd_count': 8},\n",
       " 'total': {'total_late_amd_count': 192, 'total_non_late_amd_count': 7237}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_per_length_statistics(longitudinal_sequential_prediction_timedelta2_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_sequential_prediction_timedelta2_data_dict_min5.pkl\", longitudinal_sequential_prediction_timedelta2_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_sequential_prediction_timedelta5_data_dict.pkl\", longitudinal_sequential_prediction_timedelta5_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert longitudinal prediction dictionary to ResNet test dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudinal_sequential_prediction_timedelta2_data_dict = load_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_sequential_prediction_timedelta2_data_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudinal_sequential_prediction_timedelta5_data_dict = load_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_sequential_prediction_timedelta5_data_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_longitudinal_sequential_prediction_data_dict_for_resnet_test(longitudinal_sequential_prediction_data_dict, include_per_length_test_set=True):\n",
    "    \n",
    "    validation_set_eye_list = longitudinal_sequential_prediction_data_dict[\"validation_set\"][\"eye_list\"]\n",
    "    validation_set_label_list = longitudinal_sequential_prediction_data_dict[\"validation_set\"][\"label_list\"]\n",
    "    test_set_eye_list = longitudinal_sequential_prediction_data_dict[\"test_set\"][\"eye_list\"]\n",
    "    test_set_label_list = longitudinal_sequential_prediction_data_dict[\"test_set\"][\"label_list\"]\n",
    "    \n",
    "    converted_validation_set = dict()\n",
    "    converted_test_set = dict()\n",
    "\n",
    "    for eye_list, label_list in zip(validation_set_eye_list, validation_set_label_list):\n",
    "        converted_validation_set[eye_list[-1]] = label_list[-1]\n",
    "        \n",
    "    for eye_list, label_list in zip(test_set_eye_list, test_set_label_list):\n",
    "        converted_test_set[eye_list[-1]] = label_list[-1]\n",
    "        \n",
    "    if include_per_length_test_set:\n",
    "        \n",
    "        per_length_test_set = longitudinal_sequential_prediction_data_dict[\"per_length_test_set\"]\n",
    "        converted_per_len_test_eye_dict = dict()\n",
    "        \n",
    "        for length, length_dict in per_length_test_set.items():\n",
    "            this_length_eye_list = length_dict[\"eye_list\"]\n",
    "            this_length_label_list = length_dict[\"label_list\"]\n",
    "            \n",
    "            converted_this_length_test_set = dict()\n",
    "            \n",
    "            for eye_list, label_list in zip(this_length_eye_list, this_length_label_list):\n",
    "                converted_this_length_test_set[eye_list[-1]] = label_list[-1]\n",
    "                \n",
    "            converted_per_len_test_eye_dict[length] = converted_this_length_test_set\n",
    "            \n",
    "        return {\"validation_set\" : converted_validation_set, \"test_set\" : converted_test_set, \"per_length_test_set\" : converted_per_len_test_eye_dict}\n",
    "\n",
    "    else:\n",
    "        return {\"validation_set\" : converted_validation_set, \"test_set\" : converted_test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudinal_sequential_prediction_timedelta2_data_dict_resnet_test = convert_longitudinal_sequential_prediction_data_dict_for_resnet_test(longitudinal_sequential_prediction_timedelta2_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudinal_sequential_prediction_timedelta5_data_dict_resnet_test = convert_longitudinal_sequential_prediction_data_dict_for_resnet_test(longitudinal_sequential_prediction_timedelta5_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_sequential_prediction_timedelta2_data_dict_resnet_test.pkl\", longitudinal_sequential_prediction_timedelta2_data_dict_resnet_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_sequential_prediction_timedelta5_data_dict_resnet_test.pkl\", longitudinal_sequential_prediction_timedelta5_data_dict_resnet_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-workspace",
   "language": "python",
   "name": "python3-workspace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
