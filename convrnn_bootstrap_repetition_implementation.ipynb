{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.convrnn_main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = list(range(0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data...\n",
      "1-th repetition...\n",
      "using the full train set...\n",
      "build and initialize models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jl5307/current_research/AMD_prediction/utils/convrnn_utils.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  mydata1_array = np.array(mydata1)\n",
      "/home/jl5307/current_research/AMD_prediction/utils/convrnn_utils.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  mydata2_array = np.array(mydata2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss: 0.040218\n",
      "   1/1087 [..............................] - ETA: 2:48:21batch loss: 0.036702\n",
      "batch loss: 0.044502\n",
      "   3/1087 [..............................] - ETA: 35s    batch loss: 0.031630\n",
      "batch loss: 0.032605\n",
      "   5/1087 [..............................] - ETA: 35sbatch loss: 0.027902\n",
      "batch loss: 0.023863\n",
      "   7/1087 [..............................] - ETA: 34sbatch loss: 0.021318\n",
      "batch loss: 0.025420\n",
      "   9/1087 [..............................] - ETA: 33sbatch loss: 0.018665\n",
      "batch loss: 0.014185\n",
      "  11/1087 [..............................] - ETA: 33sbatch loss: 0.019332\n",
      "batch loss: 0.023610\n",
      "  13/1087 [..............................] - ETA: 32sbatch loss: 0.029652\n",
      "batch loss: 0.243857\n",
      "  15/1087 [..............................] - ETA: 31sbatch loss: 0.015822\n",
      "batch loss: 0.019318\n",
      "  17/1087 [..............................] - ETA: 31sbatch loss: 0.014297\n",
      "batch loss: 0.021463\n",
      "  19/1087 [..............................] - ETA: 32sbatch loss: 0.016062\n",
      "batch loss: 0.014762\n",
      "  21/1087 [..............................] - ETA: 32sbatch loss: 0.024490\n",
      "batch loss: 0.019797\n",
      "  23/1087 [..............................] - ETA: 32sbatch loss: 0.018085\n",
      "batch loss: 0.013222\n",
      "  25/1087 [..............................] - ETA: 32sbatch loss: 0.015401\n",
      "batch loss: 0.012152\n",
      "  27/1087 [..............................] - ETA: 32sbatch loss: 0.014732\n",
      "batch loss: 0.060072\n",
      "  29/1087 [..............................] - ETA: 32sbatch loss: 0.020279\n",
      "batch loss: 0.010359\n",
      "  31/1087 [..............................] - ETA: 32sbatch loss: 0.074284\n",
      "batch loss: 0.019367\n",
      "  33/1087 [..............................] - ETA: 32sbatch loss: 0.027791\n",
      "batch loss: 0.056104\n",
      "  35/1087 [..............................] - ETA: 32sbatch loss: 0.044784\n",
      "batch loss: 0.022353\n",
      "  37/1087 [>.............................] - ETA: 32sbatch loss: 0.033035\n",
      "batch loss: 0.018178\n",
      "  39/1087 [>.............................] - ETA: 32sbatch loss: 0.022908\n",
      "batch loss: 0.027232\n",
      "  41/1087 [>.............................] - ETA: 31sbatch loss: 0.020326\n",
      "batch loss: 0.034728\n",
      "  43/1087 [>.............................] - ETA: 32sbatch loss: 0.038845\n",
      "batch loss: 0.043995\n",
      "  45/1087 [>.............................] - ETA: 31sbatch loss: 0.027482\n",
      "batch loss: 0.050535\n",
      "  47/1087 [>.............................] - ETA: 32sbatch loss: 0.051244\n",
      "batch loss: 0.042157\n",
      "  49/1087 [>.............................] - ETA: 31sbatch loss: 0.024508\n",
      "batch loss: 0.042770\n",
      "  51/1087 [>.............................] - ETA: 31sbatch loss: 0.021069\n",
      "batch loss: 0.020833\n",
      "  53/1087 [>.............................] - ETA: 31sbatch loss: 0.027883\n",
      "batch loss: 0.026109\n",
      "  55/1087 [>.............................] - ETA: 31sbatch loss: 0.026215\n",
      "batch loss: 0.022574\n",
      "  57/1087 [>.............................] - ETA: 31sbatch loss: 0.018495\n",
      "batch loss: 0.023656\n",
      "  59/1087 [>.............................] - ETA: 31sbatch loss: 0.010033\n",
      "batch loss: 0.036025\n",
      "  61/1087 [>.............................] - ETA: 31sbatch loss: 0.023689\n",
      "batch loss: 0.041060\n",
      "  63/1087 [>.............................] - ETA: 30sbatch loss: 0.016581\n",
      "batch loss: 0.017220\n",
      "  65/1087 [>.............................] - ETA: 30sbatch loss: 0.032382\n",
      "batch loss: 0.020563\n",
      "  67/1087 [>.............................] - ETA: 30sbatch loss: 0.023367\n",
      "batch loss: 0.015149\n",
      "  69/1087 [>.............................] - ETA: 30sbatch loss: 0.015080\n",
      "batch loss: 0.014171\n",
      "  71/1087 [>.............................] - ETA: 30sbatch loss: 0.050026\n",
      "batch loss: 0.013202\n",
      "  73/1087 [=>............................] - ETA: 30sbatch loss: 0.016144\n",
      "batch loss: 0.024083\n",
      "  75/1087 [=>............................] - ETA: 30sbatch loss: 0.009995\n",
      "batch loss: 0.016806\n",
      "  77/1087 [=>............................] - ETA: 30sbatch loss: 0.019584\n",
      "batch loss: 0.017896\n",
      "  79/1087 [=>............................] - ETA: 30sbatch loss: 0.043564\n",
      "batch loss: 0.012838\n",
      "  81/1087 [=>............................] - ETA: 30sbatch loss: 0.017100\n",
      "batch loss: 0.066527\n",
      "  83/1087 [=>............................] - ETA: 30sbatch loss: 0.023593\n",
      "batch loss: 0.015154\n",
      "  85/1087 [=>............................] - ETA: 30sbatch loss: 0.013016\n",
      "batch loss: 0.017581\n",
      "  87/1087 [=>............................] - ETA: 30sbatch loss: 0.029841\n",
      "batch loss: 0.014333\n",
      "  89/1087 [=>............................] - ETA: 30sbatch loss: 0.014942\n",
      "batch loss: 0.052231\n",
      "  91/1087 [=>............................] - ETA: 29sbatch loss: 0.013980\n",
      "batch loss: 0.037152\n",
      "  93/1087 [=>............................] - ETA: 29sbatch loss: 0.012758\n",
      "batch loss: 0.026382\n",
      "  95/1087 [=>............................] - ETA: 29sbatch loss: 0.093139\n",
      "batch loss: 0.026664\n",
      "  97/1087 [=>............................] - ETA: 29sbatch loss: 0.037949\n",
      "batch loss: 0.017610\n",
      "  99/1087 [=>............................] - ETA: 29sbatch loss: 0.019276\n",
      "batch loss: 0.047917\n",
      " 101/1087 [=>............................] - ETA: 29sbatch loss: 0.036673\n",
      "batch loss: 0.030933\n",
      " 103/1087 [=>............................] - ETA: 29sbatch loss: 0.019573\n",
      "batch loss: 0.026373\n",
      " 105/1087 [=>............................] - ETA: 29sbatch loss: 0.022689\n",
      "batch loss: 0.022063\n",
      " 107/1087 [=>............................] - ETA: 29sbatch loss: 0.020473\n",
      "batch loss: 0.029220\n",
      " 109/1087 [==>...........................] - ETA: 29sbatch loss: 0.020829\n",
      "batch loss: 0.016792\n",
      " 111/1087 [==>...........................] - ETA: 29sbatch loss: 0.035250\n",
      "batch loss: 0.026852\n",
      " 113/1087 [==>...........................] - ETA: 28sbatch loss: 0.018296\n",
      "batch loss: 0.037390\n",
      " 115/1087 [==>...........................] - ETA: 28sbatch loss: 0.025461\n",
      "batch loss: 0.014620\n",
      " 117/1087 [==>...........................] - ETA: 28sbatch loss: 0.028292\n",
      "batch loss: 0.048763\n",
      " 119/1087 [==>...........................] - ETA: 28sbatch loss: 0.034886\n",
      "batch loss: 0.020499\n",
      " 121/1087 [==>...........................] - ETA: 28sbatch loss: 0.019039\n",
      "batch loss: 0.033849\n",
      " 123/1087 [==>...........................] - ETA: 28sbatch loss: 0.012644\n",
      "batch loss: 0.012961\n",
      " 125/1087 [==>...........................] - ETA: 28sbatch loss: 0.040128\n",
      "batch loss: 0.019371\n",
      " 127/1087 [==>...........................] - ETA: 28sbatch loss: 0.015470\n",
      "batch loss: 0.012353\n",
      " 129/1087 [==>...........................] - ETA: 28sbatch loss: 0.009697\n",
      "batch loss: 0.100670\n",
      " 131/1087 [==>...........................] - ETA: 28sbatch loss: 0.024767\n",
      "batch loss: 0.023460\n",
      " 133/1087 [==>...........................] - ETA: 28sbatch loss: 0.044289\n",
      "batch loss: 0.013765\n",
      " 135/1087 [==>...........................] - ETA: 28sbatch loss: 0.016586\n",
      "batch loss: 0.040395\n",
      " 137/1087 [==>...........................] - ETA: 28sbatch loss: 0.017957\n",
      "batch loss: 0.016900\n",
      " 139/1087 [==>...........................] - ETA: 28sbatch loss: 0.036062\n",
      "batch loss: 0.014711\n",
      " 141/1087 [==>...........................] - ETA: 28sbatch loss: 0.025552\n",
      "batch loss: 0.017269\n",
      " 143/1087 [==>...........................] - ETA: 28sbatch loss: 0.016227\n",
      "batch loss: 0.042325\n",
      " 145/1087 [===>..........................] - ETA: 28sbatch loss: 0.031655\n",
      "batch loss: 0.056301\n",
      " 147/1087 [===>..........................] - ETA: 28sbatch loss: 0.016011\n",
      "batch loss: 0.019853\n",
      " 149/1087 [===>..........................] - ETA: 28sbatch loss: 0.029773\n",
      "batch loss: 0.027476\n",
      " 151/1087 [===>..........................] - ETA: 28sbatch loss: 0.029512\n",
      "batch loss: 0.034876\n",
      " 153/1087 [===>..........................] - ETA: 27sbatch loss: 0.024122\n",
      "batch loss: 0.035038\n",
      " 155/1087 [===>..........................] - ETA: 27sbatch loss: 0.022638\n",
      "batch loss: 0.018030\n",
      " 157/1087 [===>..........................] - ETA: 27sbatch loss: 0.026651\n",
      "batch loss: 0.097574\n",
      " 159/1087 [===>..........................] - ETA: 27sbatch loss: 0.016050\n",
      "batch loss: 0.018610\n",
      " 161/1087 [===>..........................] - ETA: 27sbatch loss: 0.022553\n",
      "batch loss: 0.025810\n",
      " 163/1087 [===>..........................] - ETA: 27sbatch loss: 0.017338\n",
      "batch loss: 0.016794\n",
      " 165/1087 [===>..........................] - ETA: 27sbatch loss: 0.017813\n",
      "batch loss: 0.022668\n",
      " 167/1087 [===>..........................] - ETA: 27sbatch loss: 0.031617\n",
      "batch loss: 0.021186\n",
      " 169/1087 [===>..........................] - ETA: 27sbatch loss: 0.018917\n",
      "batch loss: 0.052366\n",
      " 171/1087 [===>..........................] - ETA: 27sbatch loss: 0.027258\n",
      "batch loss: 0.015441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 173/1087 [===>..........................] - ETA: 27sbatch loss: 0.045977\n",
      "batch loss: 0.030338\n",
      " 175/1087 [===>..........................] - ETA: 27sbatch loss: 0.023264\n",
      "batch loss: 0.012988\n",
      " 177/1087 [===>..........................] - ETA: 27sbatch loss: 0.029207\n",
      "batch loss: 0.038134\n",
      " 179/1087 [===>..........................] - ETA: 27sbatch loss: 0.021193\n",
      "batch loss: 0.016205\n",
      " 181/1087 [===>..........................] - ETA: 27sbatch loss: 0.044633\n",
      "batch loss: 0.013956\n",
      " 183/1087 [====>.........................] - ETA: 27sbatch loss: 0.025334\n",
      "batch loss: 0.038898\n",
      " 185/1087 [====>.........................] - ETA: 27sbatch loss: 0.022066\n",
      "batch loss: 0.017551\n",
      " 187/1087 [====>.........................] - ETA: 26sbatch loss: 0.020152\n",
      "batch loss: 0.017101\n",
      " 189/1087 [====>.........................] - ETA: 26sbatch loss: 0.018264\n",
      "batch loss: 0.034071\n",
      " 191/1087 [====>.........................] - ETA: 26sbatch loss: 0.013308\n",
      "batch loss: 0.026871\n",
      " 193/1087 [====>.........................] - ETA: 26sbatch loss: 0.009363\n",
      "batch loss: 0.023350\n",
      " 195/1087 [====>.........................] - ETA: 26sbatch loss: 0.020033\n",
      "batch loss: 0.014384\n",
      " 197/1087 [====>.........................] - ETA: 26sbatch loss: 0.048849\n",
      "batch loss: 0.013949\n",
      " 199/1087 [====>.........................] - ETA: 26sbatch loss: 0.045361\n",
      "batch loss: 0.017168\n",
      " 201/1087 [====>.........................] - ETA: 26sbatch loss: 0.141413\n",
      "batch loss: 0.023511\n",
      " 203/1087 [====>.........................] - ETA: 26sbatch loss: 0.014200\n",
      "batch loss: 0.038881\n",
      " 205/1087 [====>.........................] - ETA: 26sbatch loss: 0.015667\n",
      "batch loss: 0.029289\n",
      " 207/1087 [====>.........................] - ETA: 26sbatch loss: 0.098619\n",
      "batch loss: 0.017851\n",
      " 209/1087 [====>.........................] - ETA: 26sbatch loss: 0.017144\n",
      "batch loss: 0.022998\n",
      " 211/1087 [====>.........................] - ETA: 26sbatch loss: 0.031168\n",
      "batch loss: 0.031802\n",
      " 213/1087 [====>.........................] - ETA: 26sbatch loss: 0.021605\n",
      "batch loss: 0.023389\n",
      " 215/1087 [====>.........................] - ETA: 26sbatch loss: 0.038508\n",
      "batch loss: 0.046360\n",
      " 217/1087 [====>.........................] - ETA: 26sbatch loss: 0.023941\n",
      "batch loss: 0.029378\n",
      " 219/1087 [=====>........................] - ETA: 25sbatch loss: 0.018947\n",
      "batch loss: 0.036120\n",
      " 221/1087 [=====>........................] - ETA: 25sbatch loss: 0.032573\n",
      "batch loss: 0.018836\n",
      " 223/1087 [=====>........................] - ETA: 25sbatch loss: 0.034957\n",
      "batch loss: 0.016076\n",
      " 225/1087 [=====>........................] - ETA: 25sbatch loss: 0.017977\n",
      "batch loss: 0.018703\n",
      " 227/1087 [=====>........................] - ETA: 25sbatch loss: 0.017942\n",
      "batch loss: 0.011589\n",
      " 229/1087 [=====>........................] - ETA: 25sbatch loss: 0.013654\n",
      "batch loss: 0.021313\n",
      " 231/1087 [=====>........................] - ETA: 25sbatch loss: 0.017516\n",
      "batch loss: 0.016453\n",
      " 233/1087 [=====>........................] - ETA: 25sbatch loss: 0.043473\n",
      "batch loss: 0.014597\n",
      " 235/1087 [=====>........................] - ETA: 25sbatch loss: 0.059252\n",
      "batch loss: 0.013303\n",
      " 237/1087 [=====>........................] - ETA: 25sbatch loss: 0.020824\n",
      "batch loss: 0.012811\n",
      " 239/1087 [=====>........................] - ETA: 25sbatch loss: 0.016953\n",
      "batch loss: 0.014039\n",
      " 241/1087 [=====>........................] - ETA: 25sbatch loss: 0.010887\n",
      "batch loss: 0.022403\n",
      " 243/1087 [=====>........................] - ETA: 25sbatch loss: 0.019972\n",
      "batch loss: 0.041596\n",
      " 245/1087 [=====>........................] - ETA: 25sbatch loss: 0.009306\n",
      "batch loss: 0.170333\n",
      " 247/1087 [=====>........................] - ETA: 25sbatch loss: 0.044698\n",
      "batch loss: 0.026871\n",
      " 249/1087 [=====>........................] - ETA: 25sbatch loss: 0.015181\n",
      "batch loss: 0.031929\n",
      " 251/1087 [=====>........................] - ETA: 25sbatch loss: 0.022025\n",
      "batch loss: 0.128964\n",
      " 253/1087 [=====>........................] - ETA: 24sbatch loss: 0.020093\n",
      "batch loss: 0.011883\n",
      " 255/1087 [======>.......................] - ETA: 24sbatch loss: 0.023217\n",
      "batch loss: 0.010077\n",
      " 257/1087 [======>.......................] - ETA: 24sbatch loss: 0.030218\n",
      "batch loss: 0.015427\n",
      " 259/1087 [======>.......................] - ETA: 24sbatch loss: 0.015810\n",
      "batch loss: 0.079807\n",
      " 261/1087 [======>.......................] - ETA: 24sbatch loss: 0.024642\n",
      "batch loss: 0.021540\n",
      " 263/1087 [======>.......................] - ETA: 24sbatch loss: 0.025618\n",
      "batch loss: 0.019376\n",
      " 265/1087 [======>.......................] - ETA: 24sbatch loss: 0.078260\n",
      "batch loss: 0.010956\n",
      " 267/1087 [======>.......................] - ETA: 24sbatch loss: 0.017975\n",
      "batch loss: 0.025594\n",
      " 269/1087 [======>.......................] - ETA: 24sbatch loss: 0.022372\n",
      "batch loss: 0.025131\n",
      " 271/1087 [======>.......................] - ETA: 24sbatch loss: 0.023760\n",
      "batch loss: 0.027811\n",
      " 273/1087 [======>.......................] - ETA: 24sbatch loss: 0.018537\n",
      "batch loss: 0.020904\n",
      " 275/1087 [======>.......................] - ETA: 24sbatch loss: 0.016276\n",
      "batch loss: 0.015247\n",
      " 277/1087 [======>.......................] - ETA: 24sbatch loss: 0.015353\n",
      "batch loss: 0.052776\n",
      " 279/1087 [======>.......................] - ETA: 24sbatch loss: 0.016467\n",
      "batch loss: 0.018903\n",
      " 281/1087 [======>.......................] - ETA: 24sbatch loss: 0.015690\n",
      "batch loss: 0.040569\n",
      " 283/1087 [======>.......................] - ETA: 24sbatch loss: 0.027188\n",
      "batch loss: 0.012587\n",
      " 285/1087 [======>.......................] - ETA: 23sbatch loss: 0.015497\n",
      "batch loss: 0.036841\n",
      " 287/1087 [======>.......................] - ETA: 23sbatch loss: 0.018463\n",
      "batch loss: 0.017374\n",
      " 289/1087 [======>.......................] - ETA: 23sbatch loss: 0.057996\n",
      "batch loss: 0.030240\n",
      " 291/1087 [=======>......................] - ETA: 23sbatch loss: 0.011046\n",
      "batch loss: 0.041306\n",
      " 293/1087 [=======>......................] - ETA: 23sbatch loss: 0.026451\n",
      "batch loss: 0.016482\n",
      " 295/1087 [=======>......................] - ETA: 23sbatch loss: 0.010215\n",
      "batch loss: 0.032478\n",
      " 297/1087 [=======>......................] - ETA: 23sbatch loss: 0.035837\n",
      "batch loss: 0.033549\n",
      " 299/1087 [=======>......................] - ETA: 23sbatch loss: 0.009059\n",
      "batch loss: 0.031777\n",
      " 301/1087 [=======>......................] - ETA: 23sbatch loss: 0.011553\n",
      "batch loss: 0.039144\n",
      " 303/1087 [=======>......................] - ETA: 23sbatch loss: 0.015725\n",
      "batch loss: 0.014416\n",
      " 305/1087 [=======>......................] - ETA: 23sbatch loss: 0.033908\n",
      "batch loss: 0.056255\n",
      " 307/1087 [=======>......................] - ETA: 23sbatch loss: 0.016306\n",
      "batch loss: 0.014961\n",
      " 309/1087 [=======>......................] - ETA: 23sbatch loss: 0.043459\n",
      "batch loss: 0.060128\n",
      " 311/1087 [=======>......................] - ETA: 23sbatch loss: 0.024115\n",
      "batch loss: 0.012025\n",
      " 313/1087 [=======>......................] - ETA: 23sbatch loss: 0.022390\n",
      "batch loss: 0.070189\n",
      " 315/1087 [=======>......................] - ETA: 23sbatch loss: 0.024341\n",
      "batch loss: 0.017902\n",
      " 317/1087 [=======>......................] - ETA: 22sbatch loss: 0.026303\n",
      "batch loss: 0.038202\n",
      " 319/1087 [=======>......................] - ETA: 22sbatch loss: 0.021203\n",
      "batch loss: 0.022073\n",
      " 321/1087 [=======>......................] - ETA: 22sbatch loss: 0.079633\n",
      "batch loss: 0.061729\n",
      " 323/1087 [=======>......................] - ETA: 22sbatch loss: 0.022606\n",
      "batch loss: 0.030328\n",
      " 325/1087 [=======>......................] - ETA: 22sbatch loss: 0.021226\n",
      "batch loss: 0.017656\n",
      " 327/1087 [========>.....................] - ETA: 22sbatch loss: 0.038265\n",
      "batch loss: 0.019787\n",
      " 329/1087 [========>.....................] - ETA: 22sbatch loss: 0.042599\n",
      "batch loss: 0.027959\n",
      " 331/1087 [========>.....................] - ETA: 22s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-350d2b109172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                              \u001b[0;34m\"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_sequential_prediction_td2_min5_data_dict_unrolled.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                              \u001b[0;34m\"/home/jl5307/current_research/AMD_prediction/results/resnet101_binary/b16_lr0005_e20_detection_imagenet/extracted_feature_dict.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                              10, 20, 32, 256, 2048, 0.0005, 0.0001, [0.03, 0.97], lr_scheduling=[2000, 0.9], seed_list=None)\n\u001b[0m",
      "\u001b[0;32m~/current_research/AMD_prediction/src/convrnn_main.py\u001b[0m in \u001b[0;36mbootstrap_train_lstm\u001b[0;34m(output_path, data_dict_path, feature_dict_path, repetition, epoch, batch_size, units, feature_dim, learning_rate, l2_reg, class_weight, lr_scheduling, seed_list)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m                     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size, max_len, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                     \u001b[0my_batch_flatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat_flatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_with_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                     \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWBCE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_batch_flatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_hat_flatten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/current_research/virtualenvs/python3-workspace/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/current_research/AMD_prediction/src/convrnn_src.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasking_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size, max_len, units)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size, max_len, units)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size, max_len, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/current_research/virtualenvs/python3-workspace/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/current_research/AMD_prediction/src/convrnn_src.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/current_research/virtualenvs/python3-workspace/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/current_research/virtualenvs/python3-workspace/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/current_research/virtualenvs/python3-workspace/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   1257\u001b[0m                (device_type is None and config.list_logical_devices('GPU'))) and\n\u001b[1;32m   1258\u001b[0m               (mask is None or\n\u001b[0;32m-> 1259\u001b[0;31m                is_cudnn_supported_inputs(mask, self.time_major)))\n\u001b[0m\u001b[1;32m   1260\u001b[0m           \u001b[0;31m# Under eager context, check the device placement and prefer the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m           \u001b[0;31m# GPU implementation when GPU is available.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/current_research/virtualenvs/python3-workspace/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mis_cudnn_supported_inputs\u001b[0;34m(mask, time_major)\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m   return math_ops.logical_and(\n\u001b[0;32m-> 1708\u001b[0;31m       \u001b[0mis_sequence_right_padded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m       math_ops.logical_not(has_fully_masked_sequence(mask)))\n\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/current_research/virtualenvs/python3-workspace/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mis_sequence_right_padded\u001b[0;34m(mask)\u001b[0m\n\u001b[1;32m   1685\u001b[0m   right_padded_mask = array_ops.sequence_mask(\n\u001b[1;32m   1686\u001b[0m       count_of_true, maxlen=max_seq_length)\n\u001b[0;32m-> 1687\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_padded_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bootstrap_train_lstm(\"/home/jl5307/current_research/AMD_prediction/results/convlstm/td2/convlstm_u256_b32_e20_r10/\", \n",
    "                             \"/home/jl5307/current_research/AMD_prediction/img_data/data_dictionary/longitudinal_sequential_prediction_td2_min5_data_dict_unrolled.pkl\",\n",
    "                             \"/home/jl5307/current_research/AMD_prediction/results/resnet101_binary/b16_lr0005_e20_detection_imagenet/extracted_feature_dict.pkl\",\n",
    "                             10, 20, 32, 256, 2048, 0.0005, 0.0001, [0.03, 0.97], lr_scheduling=[2000, 0.9], seed_list=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-workspace",
   "language": "python",
   "name": "python3-workspace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
